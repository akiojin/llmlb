# Vision Tests Workflow
# æ©Ÿèƒ½ID: SPEC-f8e3a1b7, SPEC-e03a404c
# ç›®çš„: Vision API ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆLLaVA ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
# è¦ä»¶: T028 (ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰), T030 (Vision ãƒ†ã‚¹ãƒˆæœ‰åŠ¹åŒ–)

name: Vision Tests

on:
  workflow_dispatch:
    inputs:
      model_variant:
        description: 'LLaVA model variant'
        required: false
        default: 'Q4_K_M'
        type: choice
        options:
          - Q4_K_M
          - Q8_0
      skip_model_cache:
        description: 'Skip model cache (re-download)'
        required: false
        default: false
        type: boolean
  # Visioné–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´æ™‚ã«ã‚‚å®Ÿè¡Œ
  pull_request:
    branches:
      - develop
    paths:
      - 'llmlb/src/models/image.rs'
      - 'llmlb/src/api/openai.rs'
      - 'llmlb/tests/**/vision*.rs'
      - 'xllm/src/core/vision_processor.cpp'
      - '.github/workflows/vision-tests.yml'

env:
  # LLaVA-1.5-7B GGUF model from HuggingFace
  MODEL_REPO: "mys/ggml_llava-v1.5-7b"
  MODEL_FILE: "ggml-model-q4_k.gguf"
  MMPROJ_FILE: "mmproj-model-f16.gguf"
  MODEL_CACHE_KEY: "llava-1.5-7b-q4_k-v2"

jobs:
  vision-test:
    name: Vision API Tests
    runs-on: [self-hosted, gpu]
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      # Self-hosted runner: Tools (Rust, CMake, Python) are pre-installed
      - name: Verify toolchain and architecture
        id: verify
        run: |
          echo "ðŸ” Verifying toolchain..."
          rustc --version
          cargo --version
          cmake --version
          python3 --version

          # Check for architecture mismatch on macOS
          if [[ "$OSTYPE" == "darwin"* ]]; then
            PROCESS_ARCH=$(uname -m)
            echo "Process architecture: $PROCESS_ARCH"

            # Check if running under Rosetta with ARM Homebrew
            if [[ "$PROCESS_ARCH" == "x86_64" ]] && [[ -d "/opt/homebrew" ]]; then
              echo "âš ï¸ Architecture mismatch detected!"
              echo "Process is x86_64 (Rosetta) but Homebrew is ARM64 (/opt/homebrew)"
              echo "Please re-register the runner as native ARM64"
              echo "skip_build=true" >> $GITHUB_OUTPUT
            else
              echo "skip_build=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "skip_build=false" >> $GITHUB_OUTPUT
          fi

          echo "âœ… Toolchain verified"

      - name: Skip if architecture mismatch
        if: steps.verify.outputs.skip_build == 'true'
        run: |
          echo "::warning::Skipping Vision Tests due to architecture mismatch (Rosetta + ARM Homebrew)"
          echo "Please re-register the self-hosted runner as native ARM64"
          exit 0

      - name: Bootstrap pest_generator
        if: steps.verify.outputs.skip_build != 'true'
        run: cargo build --manifest-path vendor/pest_generator/bootstrap/Cargo.toml

      - name: Ensure huggingface-hub installed
        if: steps.verify.outputs.skip_build != 'true'
        run: |
          # Check if already installed
          if python3 -c "import huggingface_hub" 2>/dev/null; then
            echo "âœ… huggingface-hub already installed"
          else
            echo "ðŸ“¦ Installing huggingface-hub..."
            # Use --break-system-packages for PEP 668 compatibility (Homebrew Python)
            pip3 install --user huggingface-hub --break-system-packages 2>/dev/null || \
            pip3 install --user huggingface-hub || \
            python3 -m pip install --user huggingface-hub --break-system-packages
          fi
          python3 -c "import huggingface_hub; print(f'huggingface-hub version: {huggingface_hub.__version__}')"

      - name: Download LLaVA model (if needed)
        if: steps.verify.outputs.skip_build != 'true'
        run: |
          MODEL_DIR="$HOME/.cache/llmlb/models"
          MODEL_PATH="$MODEL_DIR/${{ env.MODEL_FILE }}"
          MMPROJ_PATH="$MODEL_DIR/${{ env.MMPROJ_FILE }}"

          if [ -f "$MODEL_PATH" ] && [ -f "$MMPROJ_PATH" ]; then
            echo "âœ… Model files already exist"
            ls -la "$MODEL_DIR"
          else
            echo "ðŸ“¥ Downloading LLaVA model..."
            mkdir -p "$MODEL_DIR"
            python3 << 'PYEOF'
          from huggingface_hub import hf_hub_download
          import os

          model_dir = os.path.expanduser("~/.cache/llmlb/models")
          repo_id = "${{ env.MODEL_REPO }}"

          print(f"Downloading from {repo_id}...")
          hf_hub_download(repo_id=repo_id, filename="${{ env.MODEL_FILE }}", local_dir=model_dir, local_dir_use_symlinks=False)
          hf_hub_download(repo_id=repo_id, filename="${{ env.MMPROJ_FILE }}", local_dir=model_dir, local_dir_use_symlinks=False)
          print("Download complete!")
          PYEOF
            ls -la "$MODEL_DIR"
          fi

      - name: Verify model files
        if: steps.verify.outputs.skip_build != 'true'
        run: |
          echo "âœ… Verifying model files..."
          test -f ~/.cache/llmlb/models/${{ env.MODEL_FILE }} || (echo "Model file not found" && exit 1)
          test -f ~/.cache/llmlb/models/${{ env.MMPROJ_FILE }} || (echo "MMProj file not found" && exit 1)
          echo "Model files verified successfully"

      - name: Build xLLM (GPU mode)
        if: steps.verify.outputs.skip_build != 'true'
        run: |
          echo "ðŸ”¨ Building xLLM with GPU support..."

          # Clean previous build to avoid stale cache
          rm -rf xllm/build

          # Detect platform and set build options
          if [[ "$OSTYPE" == "darwin"* ]]; then
            echo "Building for macOS with Metal..."
            NPROC=$(sysctl -n hw.ncpu)
            CMAKE_OPTS="-DGGML_METAL=ON"

            # Find OpenSSL from Homebrew (ARM or Intel Mac)
            if [ -d "/opt/homebrew/opt/openssl@3" ]; then
              OPENSSL_ROOT="/opt/homebrew/opt/openssl@3"
            elif [ -d "/opt/homebrew/opt/openssl" ]; then
              OPENSSL_ROOT="/opt/homebrew/opt/openssl"
            elif [ -d "/usr/local/opt/openssl@3" ]; then
              OPENSSL_ROOT="/usr/local/opt/openssl@3"
            elif [ -d "/usr/local/opt/openssl" ]; then
              OPENSSL_ROOT="/usr/local/opt/openssl"
            fi

            if [ -n "$OPENSSL_ROOT" ]; then
              echo "Found OpenSSL at: $OPENSSL_ROOT"
              CMAKE_OPTS="$CMAKE_OPTS -DOPENSSL_ROOT_DIR=$OPENSSL_ROOT -DCMAKE_PREFIX_PATH=$OPENSSL_ROOT"
              export PKG_CONFIG_PATH="$OPENSSL_ROOT/lib/pkgconfig:$PKG_CONFIG_PATH"
              export LDFLAGS="-L$OPENSSL_ROOT/lib $LDFLAGS"
              export CPPFLAGS="-I$OPENSSL_ROOT/include $CPPFLAGS"
              export LIBRARY_PATH="$OPENSSL_ROOT/lib:$LIBRARY_PATH"
            fi
          else
            echo "Building for Linux..."
            NPROC=$(nproc)
            CMAKE_OPTS="-DGGML_CUDA=ON"
          fi

          # Force ARM64 on Apple Silicon (in case running under Rosetta)
          if [[ "$OSTYPE" == "darwin"* ]]; then
            # Check actual CPU architecture
            ACTUAL_ARCH=$(uname -m)
            echo "Actual architecture: $ACTUAL_ARCH"
            if [[ "$ACTUAL_ARCH" == "arm64" ]]; then
              CMAKE_OPTS="$CMAKE_OPTS -DCMAKE_OSX_ARCHITECTURES=arm64"
            fi
          fi

          cmake -S xllm -B xllm/build \
            -DBUILD_TESTS=OFF \
            $CMAKE_OPTS
          cmake --build xllm/build --config Release -j$NPROC

      - name: Build llmlb
        if: steps.verify.outputs.skip_build != 'true'
        run: cargo build -p llmlb

      - name: Start xLLM server with LLaVA
        if: steps.verify.outputs.skip_build != 'true'
        run: |
          echo "ðŸš€ Starting xLLM server with LLaVA model..."
          mkdir -p /tmp/xllm-logs

          ./xllm/build/xllm serve \
            --model ~/.cache/llmlb/models/${{ env.MODEL_FILE }} \
            --model-name llava-v1.5-7b \
            --mmproj ~/.cache/llmlb/models/${{ env.MMPROJ_FILE }} \
            --host 127.0.0.1 \
            --port 8080 \
            --ctx-size 2048 \
            > /tmp/xllm-logs/server.log 2>&1 &

          # Wait for xLLM to be ready
          echo "Waiting for xLLM server..."
          for i in {1..90}; do
            if curl -s http://127.0.0.1:8080/v1/models > /dev/null 2>&1; then
              echo "xLLM server is ready"
              break
            fi
            if [ $i -eq 90 ]; then
              echo "xLLM server failed to start"
              cat /tmp/xllm-logs/server.log
              exit 1
            fi
            sleep 2
          done

      - name: Start llmlb server
        if: steps.verify.outputs.skip_build != 'true'
        run: |
          echo "ðŸš€ Starting llmlb server..."
          mkdir -p /tmp/llmlb-data

          LLMLB_DATABASE_URL=sqlite:/tmp/llmlb-data/router.db \
          LLMLB_LOG_DIR=/tmp/llmlb-data/logs \
          ADMIN_USERNAME=admin \
          ADMIN_PASSWORD=test \
          ./target/debug/llmlb &

          # Wait for llmlb to be ready
          for i in {1..30}; do
            if curl -s http://127.0.0.1:32768/health > /dev/null 2>&1; then
              echo "llmlb server is ready"
              break
            fi
            sleep 2
          done

      - name: Register xLLM endpoint
        if: steps.verify.outputs.skip_build != 'true'
        run: |
          echo "ðŸ“ Registering xLLM endpoint..."
          curl -X POST http://127.0.0.1:32768/api/endpoints \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer sk_debug" \
            -d '{
              "name": "xllm-vision",
              "base_url": "http://127.0.0.1:8080"
            }'

          # Wait for model sync
          sleep 5

          # Verify models are registered
          curl -s http://127.0.0.1:32768/v1/models \
            -H "Authorization: Bearer sk_debug" | jq .

      - name: Run Vision contract tests
        if: steps.verify.outputs.skip_build != 'true'
        run: |
          set -o pipefail
          echo "ðŸ§ª Running Vision contract tests..."
          # NOTE: TDD RED tests (error handling, E2E flow) are ignored until implementation is complete
          # Integration tests that work with real LLaVA model are NOT ignored and will run
          cargo test vision --no-fail-fast 2>&1 | tee /tmp/vision-test-results.log
        env:
          RUST_LOG: debug

      - name: Upload test results
        if: always() && steps.verify.outputs.skip_build != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: vision-test-results
          path: |
            /tmp/vision-test-results.log
            /tmp/xllm-logs/
            /tmp/llmlb-data/logs/
          retention-days: 7

      - name: Cleanup
        if: always() && steps.verify.outputs.skip_build != 'true'
        run: |
          pkill -f xllm || true
          pkill -f llmlb || true
