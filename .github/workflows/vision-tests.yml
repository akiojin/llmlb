# Vision Tests Workflow
# æ©Ÿèƒ½ID: SPEC-f8e3a1b7, SPEC-e03a404c
# ç›®çš„: Vision API ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆLLaVA ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
# è¦ä»¶: T028 (ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰), T030 (Vision ãƒ†ã‚¹ãƒˆæœ‰åŠ¹åŒ–)

name: Vision Tests

on:
  workflow_dispatch:
    inputs:
      model_variant:
        description: 'LLaVA model variant'
        required: false
        default: 'Q4_K_M'
        type: choice
        options:
          - Q4_K_M
          - Q8_0
      skip_model_cache:
        description: 'Skip model cache (re-download)'
        required: false
        default: false
        type: boolean
  # Visioné–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´æ™‚ã«ã‚‚å®Ÿè¡Œ
  pull_request:
    branches:
      - develop
    paths:
      - 'llmlb/src/models/image.rs'
      - 'llmlb/src/api/openai.rs'
      - 'llmlb/tests/**/vision*.rs'
      - 'xllm/src/core/vision_processor.cpp'
      - '.github/workflows/vision-tests.yml'

env:
  # LLaVA-1.5-7B GGUF model from HuggingFace
  MODEL_REPO: "mys/ggml_llava-v1.5-7b"
  MODEL_FILE: "ggml-model-q4_k.gguf"
  MMPROJ_FILE: "mmproj-model-f16.gguf"
  MODEL_CACHE_KEY: "llava-1.5-7b-q4_k-v2"

jobs:
  vision-test:
    name: Vision API Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    # NOTE: This job is experimental until xLLM 'serve' command is implemented
    # Currently xLLM serve returns "Error: serve command not yet implemented"
    continue-on-error: true

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: Setup C++ build environment
        uses: ./.github/actions/setup-cpp

      - name: Bootstrap pest_generator
        run: cargo build --manifest-path vendor/pest_generator/bootstrap/Cargo.toml

      # LLaVA ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆç´„ 4GBï¼‰
      - name: Cache LLaVA model
        if: ${{ github.event.inputs.skip_model_cache != 'true' }}
        id: cache-model
        uses: actions/cache@v4
        with:
          path: ~/.cache/llmlb/models
          key: ${{ env.MODEL_CACHE_KEY }}-${{ env.MODEL_FILE }}-${{ env.MMPROJ_FILE }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install huggingface-hub
        run: pip install huggingface-hub

      - name: Download LLaVA model
        if: steps.cache-model.outputs.cache-hit != 'true'
        run: |
          echo "ğŸ“¥ Downloading LLaVA model..."
          mkdir -p ~/.cache/llmlb/models

          # Download model files using Python script
          python << 'PYEOF'
          from huggingface_hub import hf_hub_download
          import os

          model_dir = os.path.expanduser("~/.cache/llmlb/models")

          print("Downloading model file...")
          hf_hub_download(
              repo_id="${{ env.MODEL_REPO }}",
              filename="${{ env.MODEL_FILE }}",
              local_dir=model_dir,
              local_dir_use_symlinks=False
          )

          print("Downloading mmproj file...")
          hf_hub_download(
              repo_id="${{ env.MODEL_REPO }}",
              filename="${{ env.MMPROJ_FILE }}",
              local_dir=model_dir,
              local_dir_use_symlinks=False
          )

          print("Download complete!")
          PYEOF

          ls -la ~/.cache/llmlb/models/

      - name: Verify model files
        run: |
          echo "âœ… Verifying model files..."
          test -f ~/.cache/llmlb/models/${{ env.MODEL_FILE }} || (echo "Model file not found" && exit 1)
          test -f ~/.cache/llmlb/models/${{ env.MMPROJ_FILE }} || (echo "MMProj file not found" && exit 1)
          echo "Model files verified successfully"

      - name: Cache ccache
        uses: actions/cache@v4
        with:
          path: ~/.cache/ccache
          key: ${{ runner.os }}-ccache-vision-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-ccache-vision-

      - name: Build llama-server (CPU mode)
        run: |
          echo "ğŸ”¨ Building llama-server..."
          cmake -S xllm -B xllm/build \
            -DBUILD_TESTS=OFF \
            -DPORTABLE_BUILD=ON \
            -DLLAMA_NATIVE=OFF \
            -DLLAMA_BUILD_TOOLS=ON
          cmake --build xllm/build --config Release --target llama-server -j$(nproc)

      - name: Build llmlb
        run: cargo build -p llmlb

      - name: Start llama-server with LLaVA
        run: |
          echo "ğŸš€ Starting llama-server with LLaVA model..."
          mkdir -p /tmp/xllm-logs

          ./xllm/build/bin/llama-server \
            --model ~/.cache/llmlb/models/${{ env.MODEL_FILE }} \
            --mmproj ~/.cache/llmlb/models/${{ env.MMPROJ_FILE }} \
            --host 127.0.0.1 \
            --port 8080 \
            --ctx-size 2048 \
            > /tmp/xllm-logs/server.log 2>&1 &

          # Wait for llama-server to be ready
          echo "Waiting for llama-server..."
          for i in {1..90}; do
            if curl -s http://127.0.0.1:8080/v1/models > /dev/null 2>&1; then
              echo "llama-server is ready"
              break
            fi
            if [ $i -eq 90 ]; then
              echo "llama-server failed to start"
              cat /tmp/xllm-logs/server.log
              exit 1
            fi
            sleep 2
          done

      - name: Start llmlb server
        run: |
          echo "ğŸš€ Starting llmlb server..."
          mkdir -p /tmp/llmlb-data

          LLMLB_DATABASE_URL=sqlite:/tmp/llmlb-data/router.db \
          LLMLB_LOG_DIR=/tmp/llmlb-data/logs \
          ADMIN_USERNAME=admin \
          ADMIN_PASSWORD=test \
          ./target/debug/llmlb &

          # Wait for llmlb to be ready
          for i in {1..30}; do
            if curl -s http://127.0.0.1:32768/health > /dev/null 2>&1; then
              echo "llmlb server is ready"
              break
            fi
            sleep 2
          done

      - name: Register llama-server endpoint
        run: |
          echo "ğŸ“ Registering llama-server endpoint..."
          curl -X POST http://127.0.0.1:32768/api/endpoints \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer sk_debug" \
            -d '{
              "name": "llama-vision",
              "base_url": "http://127.0.0.1:8080"
            }'

          # Wait for model sync
          sleep 5

          # Verify models are registered
          curl -s http://127.0.0.1:32768/v1/models \
            -H "Authorization: Bearer sk_debug" | jq .

      - name: Run Vision contract tests
        run: |
          echo "ğŸ§ª Running Vision contract tests..."
          cargo test vision --no-fail-fast -- --include-ignored 2>&1 | tee /tmp/vision-test-results.log
        env:
          RUST_LOG: debug

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: vision-test-results
          path: |
            /tmp/vision-test-results.log
            /tmp/xllm-logs/
            /tmp/llmlb-data/logs/
          retention-days: 7

      - name: Cleanup
        if: always()
        run: |
          pkill -f llama-server || true
          pkill -f llmlb || true
