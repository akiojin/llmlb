# llmlb アーキテクチャ設計方針

本ドキュメントはllmlbの設計方針を定義します。

## 1. llmlbの役割: ゲートウェイ

llmlbは**APIゲートウェイ**として機能します。

- エンドポイント（xLLM、Ollama、vLLM等）を**ブラックボックス**として扱う
- エンドポイントの内部状態（VRAM状況、モデルロード状態等）は把握しない
- 内部リソース管理はエンドポイント側の責任

### ゲートウェイの責務

| 責務 | 説明 |
|------|------|
| リクエストルーティング | クライアントからのリクエストを適切なエンドポイントに転送 |
| ヘルスチェック | エンドポイント単位の死活監視 |
| 負荷分散 | 複数エンドポイント間でのリクエスト分散 |
| 認証 | JWT認証（ダッシュボード）、APIキー認証（API） |
| モデル一覧 | エンドポイントへのポーリングで自動取得 |

### ゲートウェイが行わないこと

| 項目 | 理由 |
|------|------|
| VRAM状況の監視 | エンドポイント内部の詳細 |
| モデルロード状態の管理 | エンドポイント側の責任 |
| 推論キューの管理 | エンドポイント側の責任 |
| モデル単位のヘルスチェック | エンドポイント単位で十分 |

## 2. エンドポイント管理

### 登録情報

エンドポイント登録に必要な情報は最小限に抑えます：

| 項目 | 必須/任意 | 説明 |
|------|-----------|------|
| URL | 必須 | エンドポイントのベースURL |
| APIキー | 任意 | 認証が必要な場合のみ |
| 名前 | 任意 | 識別用の表示名 |

### モデル一覧の取得

エンドポイントが提供するモデル一覧は、エンドポイントの`/v1/models`APIを
ポーリングして自動取得します。llmlb側でモデル情報を手動管理しません。

### ヘルスチェック

- **対象**: エンドポイント単位
- **方式**: 定期的なHTTPリクエスト（`/health`または`/v1/models`）
- **判定**: レスポンスステータスコードによるオンライン/オフライン判定

モデル単位のヘルスチェックは行いません。モデルの可用性は
エンドポイントの`/v1/models`レスポンスに委ねます。

### デバイス情報の取得

エンドポイント登録時に`/api/system`APIを試行し、デバイス情報（GPU等）を取得します。

- **対象**: 全エンドポイント
- **タイミング**: 登録時のみ（ヘルスチェック時は呼び出さない）
- **エラー時**: 無視して登録を続行（OpenAI互換エンドポイントとして扱う）

`/api/system`はxLLM固有のAPIであり、OpenAI互換APIの範囲外です。
対応していないエンドポイント（Ollama、vLLM等）では単にスキップされます。

### CPU推論の許容

GPU非搭載のエンドポイントも登録可能です。デバイス情報がない場合も
正常に動作し、レイテンシベースの負荷分散に参加します。

## 3. 負荷分散戦略: レイテンシ優先

llmlbは**レイテンシ優先**方式を採用します。

```text
エンドポイントA (latency: 100ms) ← 優先
エンドポイントB (latency: 200ms)
エンドポイントC (latency: 300ms)
```

### レイテンシ計測方式

| 項目 | 説明 |
|------|------|
| 計測タイミング | 推論リクエスト成功時 |
| 計算方式 | 指数移動平均（EMA、α=0.2） |
| 初期値 | 無限大（最低優先度） |
| オフライン時 | 無限大にリセット |
| 永続化 | SQLiteに保存 |

### タイブレーク

複数エンドポイントが同一レイテンシの場合、ラウンドロビンでタイブレークします。

### ゲートウェイ設計との整合性

レイテンシ計測は推論リクエストのレスポンス時間を利用するため、
エンドポイントの内部状態に依存せず、ブラックボックス設計を維持します。

## 4. 設計原則

### シンプルさの優先

- 複雑な最適化より予測可能な動作を優先
- エンドポイントの詳細を抽象化し、疎結合を維持
- 各コンポーネントの責務を明確に分離

### ブラックボックスとしてのエンドポイント

```text
┌─────────────────────────────────────────────────┐
│                     llmlb                        │
│  ┌─────────────────────────────────────────┐    │
│  │ APIゲートウェイ                          │    │
│  │ - ルーティング                           │    │
│  │ - 認証                                   │    │
│  │ - レイテンシ優先負荷分散                 │    │
│  └─────────────────────────────────────────┘    │
└──────────────────────┬──────────────────────────┘
                       │
       ┌───────────────┼───────────────┐
       │               │               │
       ▼               ▼               ▼
┌──────────────┐ ┌──────────────┐ ┌──────────────┐
│ エンドポイント│ │ エンドポイント│ │ エンドポイント│
│ (xLLM)       │ │ (vLLM)       │ │ (Ollama)     │
│              │ │              │ │              │
│ ブラック     │ │ ブラック     │ │ ブラック     │
│ ボックス     │ │ ボックス     │ │ ボックス     │
└──────────────┘ └──────────────┘ └──────────────┘
```

llmlbはエンドポイントの内部実装に依存せず、
OpenAI互換APIという共通インターフェースのみに依存します。
