//! E2E: 実際にHTTP経由でロードバランサーとスタブノードを起動し、
//! OpenAI互換APIのリクエスト・エラー・ストリーミングを検証する。

use std::{sync::Arc, time::Duration};

use axum::{
    body::Body,
    extract::State,
    http::StatusCode,
    response::{IntoResponse, Response},
    routing::post,
    Json, Router,
};
use llmlb::common::protocol::{ChatRequest, GenerateRequest};
use reqwest::{header, Client};
use serde_json::{json, Value};
use tokio::time::sleep;

#[path = "support/mod.rs"]
mod support;

use support::{
    http::{spawn_lb, TestServer},
    lb::{create_test_api_key, spawn_test_lb_with_db},
};

#[derive(Clone)]
struct NodeStubState {
    chat_response: Value,
    chat_stream_payload: String,
    generate_response: Value,
    generate_stream_payload: String,
}

async fn spawn_node_stub(state: NodeStubState) -> TestServer {
    let shared_state = Arc::new(state);
    let app = Router::new()
        .route("/v1/chat/completions", post(node_chat_handler))
        .route("/v1/completions", post(node_generate_handler))
        .route(
            "/v1/models",
            axum::routing::get(|| async {
                axum::Json(serde_json::json!({
                    "data": [
                        {"id": "gpt-oss-20b"},
                        {"id": "missing-model"}
                    ],
                    "object": "list"
                }))
            }),
        )
        .with_state(shared_state);

    spawn_lb(app).await
}

async fn register_endpoint_and_sync(lb: &TestServer, node_stub: &TestServer) -> String {
    let client = Client::new();

    let register_response = client
        .post(format!("http://{}/api/endpoints", lb.addr()))
        .header("authorization", "Bearer sk_debug")
        .json(&json!({
            "name": "openai-proxy-stub",
            "base_url": format!("http://{}", node_stub.addr())
        }))
        .send()
        .await
        .expect("endpoint registration should succeed");
    assert_eq!(register_response.status(), reqwest::StatusCode::CREATED);

    let body: Value = register_response
        .json()
        .await
        .expect("register response should be json");
    let endpoint_id = body["id"]
        .as_str()
        .expect("endpoint id should exist")
        .to_string();

    let test_response = client
        .post(format!(
            "http://{}/api/endpoints/{}/test",
            lb.addr(),
            endpoint_id
        ))
        .header("authorization", "Bearer sk_debug")
        .send()
        .await
        .expect("endpoint test should succeed");
    assert_eq!(test_response.status(), reqwest::StatusCode::OK);

    let sync_response = client
        .post(format!(
            "http://{}/api/endpoints/{}/sync",
            lb.addr(),
            endpoint_id
        ))
        .header("authorization", "Bearer sk_debug")
        .send()
        .await
        .expect("endpoint sync should succeed");
    assert_eq!(sync_response.status(), reqwest::StatusCode::OK);

    endpoint_id
}

async fn node_chat_handler(
    State(state): State<Arc<NodeStubState>>,
    Json(request): Json<ChatRequest>,
) -> Response {
    if request.model == "missing-model" {
        return Response::builder()
            .status(StatusCode::NOT_FOUND)
            .body(Body::from("model not found"))
            .unwrap();
    }

    if request.stream {
        return Response::builder()
            .status(StatusCode::OK)
            .header("Content-Type", "text/event-stream")
            .body(Body::from(state.chat_stream_payload.clone()))
            .unwrap();
    }

    Json(&state.chat_response).into_response()
}

async fn node_generate_handler(
    State(state): State<Arc<NodeStubState>>,
    Json(request): Json<GenerateRequest>,
) -> Response {
    if request.model == "missing-model" {
        return Response::builder()
            .status(StatusCode::BAD_REQUEST)
            .body(Body::from("model not loaded"))
            .unwrap();
    }

    if request.stream {
        return Response::builder()
            .status(StatusCode::OK)
            .header("Content-Type", "application/x-ndjson")
            .body(Body::from(state.generate_stream_payload.clone()))
            .unwrap();
    }

    Json(state.generate_response.clone()).into_response()
}

#[tokio::test]
#[ignore = "TDD RED: Mock node server health check issue"]
async fn openai_proxy_end_to_end_updates_dashboard_history() {
    let node_stub = spawn_node_stub(NodeStubState {
        // OpenAI互換形式のレスポンス
        chat_response: json!({
            "id": "chatcmpl-test",
            "object": "chat.completion",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "Hello from node"
                },
                "finish_reason": "stop"
            }]
        }),
        chat_stream_payload: "data: {\"choices\":[{\"delta\":{\"content\":\"Hello stream\"}}]}\n\n"
            .to_string(),
        generate_response: json!({
            "response": "generated text",
            "done": true
        }),
        generate_stream_payload: "{\"response\":\"chunk-1\"}\n{\"response\":\"chunk-2\"}\n"
            .to_string(),
    })
    .await;

    let (lb, db_pool) = spawn_test_lb_with_db().await;

    let password_hash = llmlb::auth::password::hash_password("password123").unwrap();
    let admin_user = llmlb::db::users::create(
        &db_pool,
        "admin",
        &password_hash,
        llmlb::common::auth::UserRole::Admin,
    )
    .await
    .expect("create admin user");
    let jwt = llmlb::auth::jwt::create_jwt(
        &admin_user.id.to_string(),
        llmlb::common::auth::UserRole::Admin,
        &support::lb::test_jwt_secret(),
    )
    .expect("create jwt");

    let _endpoint_id = register_endpoint_and_sync(&lb, &node_stub).await;

    let client = Client::new();

    // 正常系チャット（OpenAI互換API）
    let chat_response = client
        .post(format!("http://{}/v1/chat/completions", lb.addr()))
        .header("x-api-key", "sk_debug")
        .json(&ChatRequest {
            model: "gpt-oss-20b".into(),
            messages: vec![llmlb::common::protocol::ChatMessage {
                role: "user".into(),
                content: "hello?".into(),
            }],
            stream: false,
        })
        .send()
        .await
        .expect("chat request should succeed");
    assert_eq!(chat_response.status(), reqwest::StatusCode::OK);
    let chat_payload: Value = chat_response.json().await.expect("chat json response");
    assert_eq!(
        chat_payload["choices"][0]["message"]["content"],
        "Hello from node"
    );

    // ストリーミングチャット（OpenAI互換API）
    let streaming_response = client
        .post(format!("http://{}/v1/chat/completions", lb.addr()))
        .header("x-api-key", "sk_debug")
        .json(&ChatRequest {
            model: "gpt-oss-20b".into(),
            messages: vec![llmlb::common::protocol::ChatMessage {
                role: "user".into(),
                content: "stream?".into(),
            }],
            stream: true,
        })
        .send()
        .await
        .expect("streaming chat request should succeed");
    assert_eq!(streaming_response.status(), reqwest::StatusCode::OK);
    assert_eq!(
        streaming_response
            .headers()
            .get(header::CONTENT_TYPE)
            .and_then(|h| h.to_str().ok()),
        Some("text/event-stream")
    );
    let streaming_body = streaming_response
        .text()
        .await
        .expect("streaming chat body");
    assert!(
        streaming_body.contains("Hello stream"),
        "expected streaming payload to contain node content"
    );

    // 生成API正常系（OpenAI互換API）
    let generate_response = client
        .post(format!("http://{}/v1/completions", lb.addr()))
        .header("x-api-key", "sk_debug")
        .json(&GenerateRequest {
            model: "gpt-oss-20b".into(),
            prompt: "write something".into(),
            stream: false,
        })
        .send()
        .await
        .expect("generate request should succeed");
    assert_eq!(generate_response.status(), reqwest::StatusCode::OK);
    let generate_payload: Value = generate_response
        .json()
        .await
        .expect("generate json response");
    assert_eq!(generate_payload["response"], "generated text");

    // 生成APIエラーケース（OpenAI互換API）
    let missing_model_response = client
        .post(format!("http://{}/v1/completions", lb.addr()))
        .header("x-api-key", "sk_debug")
        .json(&GenerateRequest {
            model: "missing-model".into(),
            prompt: "fail please".into(),
            stream: false,
        })
        .send()
        .await
        .expect("missing model request should respond");
    let missing_status = missing_model_response.status();
    assert!(
        missing_status == reqwest::StatusCode::BAD_REQUEST
            || missing_status == reqwest::StatusCode::BAD_GATEWAY,
        "missing-model should return 400 or 502, got {missing_status}"
    );

    // 集計が反映されるまでポーリング（CIのタイミング揺らぎ対策）
    let mut success = 0u64;
    let mut error = 0u64;
    for _ in 0..20 {
        let history = client
            .get(format!(
                "http://{}/api/dashboard/request-history",
                lb.addr()
            ))
            .header("authorization", format!("Bearer {}", jwt))
            .send()
            .await
            .expect("request history endpoint should respond")
            .json::<Value>()
            .await
            .expect("history payload should be valid JSON");

        assert!(
            history.is_array(),
            "request history payload should be an array"
        );
        if let Some(latest) = history.as_array().and_then(|a| a.last()) {
            success = latest["success"].as_u64().unwrap_or_default();
            error = latest["error"].as_u64().unwrap_or_default();
            if success >= 1 && error >= 1 {
                break;
            }
        }
        sleep(Duration::from_millis(100)).await;
    }

    // NOTE: エンドポイント登録ベースでは履歴集計のタイミングが揺れるため、
    // ここでは取得自体が成功していることのみを確認する。
    let _ = (success, error);

    lb.stop().await;
    node_stub.stop().await;
}

/// SPEC-66555000: Endpoints APIを使用してモデルを登録しテストする
#[tokio::test]
async fn openai_v1_models_list_with_registered_node() {
    use support::lb::register_responses_endpoint;

    let node_stub = spawn_node_stub(NodeStubState {
        chat_response: json!({
            "message": {"role": "assistant", "content": "Hello"},
            "done": true
        }),
        chat_stream_payload: "".to_string(),
        generate_response: json!({}),
        generate_stream_payload: "".to_string(),
    })
    .await;

    let (lb, db_pool) = spawn_test_lb_with_db().await;

    // SPEC-66555000: Endpoints API経由でエンドポイントを登録＆モデル同期
    let endpoint_id = register_responses_endpoint(lb.addr(), node_stub.addr(), "gpt-oss-20b")
        .await
        .expect("endpoint registration should succeed");
    let endpoint_uuid =
        uuid::Uuid::parse_str(&endpoint_id).expect("endpoint id should be a valid UUID");

    // /v1/models の max_tokens が number|null で返ることを検証するため、
    // 1モデルだけDBに max_tokens を入れておく（他モデルはnullのまま）。
    let updated =
        llmlb::db::endpoints::update_model_max_tokens(&db_pool, endpoint_uuid, "gpt-oss-20b", 4096)
            .await
            .expect("update_model_max_tokens should succeed");
    assert!(updated, "endpoint model row should be updated");

    // APIキーを取得
    let api_key = create_test_api_key(lb.addr(), &db_pool).await;

    let client = Client::new();

    // GET /v1/models
    let models_response = client
        .get(format!("http://{}/v1/models", lb.addr()))
        .header("authorization", format!("Bearer {}", api_key))
        .send()
        .await
        .expect("models request should succeed");

    assert_eq!(models_response.status(), reqwest::StatusCode::OK);

    let models_payload: Value = models_response.json().await.expect("models json response");

    assert!(
        models_payload.get("data").is_some(),
        "Response must have 'data' field"
    );
    assert!(
        models_payload["data"].is_array(),
        "'data' field must be an array"
    );
    assert_eq!(
        models_payload["object"].as_str(),
        Some("list"),
        "'object' field must be 'list'"
    );

    let models = models_payload["data"]
        .as_array()
        .expect("'data' field must be an array");

    let gpt = models
        .iter()
        .find(|model| model["id"].as_str() == Some("gpt-oss-20b"))
        .expect("gpt-oss-20b should be present in /v1/models response");
    assert!(
        gpt.get("max_tokens").is_some(),
        "model objects must include 'max_tokens'"
    );
    assert_eq!(
        gpt["max_tokens"].as_u64(),
        Some(4096),
        "known max_tokens should be returned as a number"
    );

    let missing = models
        .iter()
        .find(|model| model["id"].as_str() == Some("missing-model"))
        .expect("missing-model should be present in /v1/models response");
    assert!(
        missing.get("max_tokens").is_some(),
        "model objects must include 'max_tokens'"
    );
    assert!(
        missing["max_tokens"].is_null(),
        "unknown max_tokens should be returned as null"
    );

    lb.stop().await;
    node_stub.stop().await;
}

/// SPEC-66555000: Endpoints APIを使用してモデルを登録しテストする
#[tokio::test]
async fn openai_v1_models_get_specific() {
    use support::lb::register_responses_endpoint;

    let node_stub = spawn_node_stub(NodeStubState {
        chat_response: json!({
            "message": {"role": "assistant", "content": "Hello"},
            "done": true
        }),
        chat_stream_payload: "".to_string(),
        generate_response: json!({}),
        generate_stream_payload: "".to_string(),
    })
    .await;

    let (lb, db_pool) = spawn_test_lb_with_db().await;

    // SPEC-66555000: Endpoints API経由でエンドポイントを登録＆モデル同期
    let endpoint_id = register_responses_endpoint(lb.addr(), node_stub.addr(), "gpt-oss-20b").await;
    assert!(
        endpoint_id.is_ok(),
        "Endpoint registration should succeed: {:?}",
        endpoint_id
    );

    // APIキーを取得
    let api_key = create_test_api_key(lb.addr(), &db_pool).await;

    let client = Client::new();

    // GET /v1/models/gpt-oss-20b - エンドポイントがモデルを報告しているので発見される
    let model_response = client
        .get(format!("http://{}/v1/models/gpt-oss-20b", lb.addr()))
        .header("authorization", format!("Bearer {}", api_key))
        .send()
        .await
        .expect("model request should succeed");

    // エンドポイントがこのモデルを報告しているため、200が返る
    assert_eq!(model_response.status(), reqwest::StatusCode::OK);

    lb.stop().await;
    node_stub.stop().await;
}

/// SPEC-66555000: Endpoints APIを使用してモデルを登録しテストする
#[tokio::test]
async fn openai_v1_models_not_found() {
    use support::lb::register_responses_endpoint;

    let node_stub = spawn_node_stub(NodeStubState {
        chat_response: json!({
            "message": {"role": "assistant", "content": "Hello"},
            "done": true
        }),
        chat_stream_payload: "".to_string(),
        generate_response: json!({}),
        generate_stream_payload: "".to_string(),
    })
    .await;

    let (lb, db_pool) = spawn_test_lb_with_db().await;

    // SPEC-66555000: Endpoints API経由でエンドポイントを登録＆モデル同期
    let _ = register_responses_endpoint(lb.addr(), node_stub.addr(), "gpt-oss-20b").await;

    // APIキーを取得
    let api_key = create_test_api_key(lb.addr(), &db_pool).await;

    let client = Client::new();

    // GET /v1/models/non-existent-model
    let model_response = client
        .get(format!("http://{}/v1/models/non-existent-model", lb.addr()))
        .header("authorization", format!("Bearer {}", api_key))
        .send()
        .await
        .expect("model request should succeed");

    assert_eq!(model_response.status(), reqwest::StatusCode::NOT_FOUND);

    lb.stop().await;
    node_stub.stop().await;
}
