[
  {
    "id": "qwen2.5-7b-instruct",
    "name": "Qwen2.5 7B Instruct",
    "description": "Alibaba's multilingual instruction-tuned model with strong reasoning capabilities",
    "repo": "bartowski/Qwen2.5-7B-Instruct-GGUF",
    "recommended_filename": "Qwen2.5-7B-Instruct-Q4_K_M.gguf",
    "size_bytes": 4920000000,
    "required_memory_bytes": 7380000000,
    "tags": ["chat", "multilingual", "coding"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "7B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "llama3.2-3b-instruct",
    "name": "Llama 3.2 3B Instruct",
    "description": "Meta's lightweight instruction-tuned model optimized for edge deployment",
    "repo": "bartowski/Llama-3.2-3B-Instruct-GGUF",
    "recommended_filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    "size_bytes": 2020000000,
    "required_memory_bytes": 3030000000,
    "tags": ["chat", "lightweight"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "3B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "mistral-7b-instruct",
    "name": "Mistral 7B Instruct",
    "description": "Mistral AI's efficient instruction-tuned model with sliding window attention",
    "repo": "bartowski/Mistral-7B-Instruct-v0.3-GGUF",
    "recommended_filename": "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf",
    "size_bytes": 4370000000,
    "required_memory_bytes": 6555000000,
    "tags": ["chat", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "7B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "phi-3-mini",
    "name": "Phi-3 Mini",
    "description": "Microsoft's compact yet powerful model for reasoning and coding",
    "repo": "bartowski/Phi-3-mini-4k-instruct-GGUF",
    "recommended_filename": "Phi-3-mini-4k-instruct-Q4_K_M.gguf",
    "size_bytes": 2390000000,
    "required_memory_bytes": 3585000000,
    "tags": ["chat", "coding", "compact"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "3.8B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "gemma-2-9b",
    "name": "Gemma 2 9B",
    "description": "Google's open model with strong performance across diverse tasks",
    "repo": "bartowski/gemma-2-9b-it-GGUF",
    "recommended_filename": "gemma-2-9b-it-Q4_K_M.gguf",
    "size_bytes": 5760000000,
    "required_memory_bytes": 8640000000,
    "tags": ["chat", "multilingual"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "9B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  }
]
