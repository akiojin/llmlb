# 機能仕様書: OpenAI互換API完全準拠

**機能ID**: `SPEC-0f1de549`
**作成日**: 2026-01-05
**更新日**: 2026-01-16
**ステータス**: 実装完了
**入力**: ユーザー説明: "OpenAI互換API完全準拠 - 内蔵エンジンのOpenAI互換APIを100%準拠にする。Open Responses API（/v1/responses）のパススルー対応を含む。"

## ユーザーシナリオ＆テスト *(必須)*

### ユーザーストーリー1 - トークン使用量の正確な取得 (優先度: P1)

開発者として、APIレスポンスから正確なトークン使用量を取得したい。
これにより、アプリケーションのコスト計算、使用量監視、クォータ管理を
正確に行うことができる。

**この優先度の理由**: トークン課金や使用量制限を行うアプリケーションにとって
最も重要な情報であり、多くのOpenAI SDKがこの値を前提としている。

**独立テスト**: Chat CompletionsまたはCompletionsエンドポイントを呼び出し、
レスポンスのusageフィールドにprompt_tokens、completion_tokens、total_tokensが
正確な値で含まれていることを確認できる。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、**実行** Chat Completionsリクエストを送信、
   **結果** レスポンスにusage.prompt_tokens、usage.completion_tokens、
   usage.total_tokensが含まれ、total_tokens = prompt_tokens + completion_tokensである

2. **前提** 既知の入力トークン数のプロンプトを用意、**実行** リクエストを送信、
   **結果** 返されたprompt_tokensが実際のトークン数と一致する（±5%以内）

3. **前提** ストリーミングモードでリクエスト、**実行** レスポンスを受信、
   **結果** 最終チャンクまたは別途usageが取得可能

---

### ユーザーストーリー2 - レスポンスIDとタイムスタンプの一意性 (優先度: P1)

開発者として、各APIレスポンスに一意のIDと正確なタイムスタンプを取得したい。
これにより、リクエストのトレーシング、ログ分析、デバッグを効率的に行うことができる。

**この優先度の理由**: 本番環境でのトラブルシューティングやログ集約において、
各レスポンスを一意に識別できることは運用上必須である。

**独立テスト**: 連続して複数のリクエストを送信し、各レスポンスのidフィールドが
異なる値であること、createdフィールドが現在時刻に近いことを確認できる。

**受け入れシナリオ**:

1. **前提** なし、**実行** 2つのChat Completionsリクエストを連続送信、
   **結果** 各レスポンスのidフィールドが異なる一意の値である

2. **前提** なし、**実行** リクエストを送信、
   **結果** createdフィールドが現在のUNIXタイムスタンプ（±5秒以内）である

3. **前提** なし、**実行** リクエストを送信、
   **結果** idフィールドが「chatcmpl-」または「cmpl-」プレフィックスで始まる

---

### ユーザーストーリー3 - 繰り返しペナルティパラメータの使用 (優先度: P2)

開発者として、presence_penaltyとfrequency_penaltyパラメータを使用して、
生成テキストの多様性を制御したい。
これにより、OpenAI APIと同じ方法で出力の繰り返しを抑制できる。

**この優先度の理由**: 多くのプロンプトエンジニアリング手法やアプリケーションが
これらのパラメータを活用しており、互換性のために重要である。

**独立テスト**: presence_penaltyやfrequency_penaltyを設定したリクエストが
エラーなく処理され、出力に影響を与えることを確認できる。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、**実行** presence_penalty=1.0でリクエスト、
   **結果** リクエストがエラーなく処理され、レスポンスが返される

2. **前提** モデルがロードされている、**実行** frequency_penalty=1.0でリクエスト、
   **結果** リクエストがエラーなく処理され、レスポンスが返される

3. **前提** なし、**実行** presence_penalty=3.0（範囲外）でリクエスト、
   **結果** 適切なエラーメッセージで400エラーが返される

---

### ユーザーストーリー4 - トークン確率（logprobs）の取得 (優先度: P2)

開発者として、生成された各トークンの確率情報を取得したい。
これにより、モデルの信頼度分析、出力品質の評価、トークン選択の
詳細な分析が可能になる。

**この優先度の理由**: 研究用途やモデル評価において重要な機能であり、
OpenAI APIの主要な機能の一つである。

**独立テスト**: logprobs=trueを指定したリクエストを送信し、各トークンの
確率情報が実際の値で返されることを確認できる。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、**実行** logprobs=trueでChat Completions、
   **結果** choices[0].logprobs.contentに各トークンの確率情報が含まれる

2. **前提** モデルがロードされている、**実行** top_logprobs=5でリクエスト、
   **結果** 各トークン位置で上位5つの候補トークンとその確率が返される

3. **前提** なし、**実行** logprobs情報を確認、
   **結果** logprobの値が負の実数（log確率）であり、0.0ではない

4. **前提** モデルがロードされている、**実行** logprobs=trueでリクエスト、
   **結果** logprobの値がllama_get_logits()から取得した実際のlogits値に基づいている
   - **実装注記（Phase 3.6）**: 初期実装ではハッシュベースの疑似値を使用。
     実値化によりモデルの実際の確信度を反映した値に置き換える。

---

### ユーザーストーリー5 - 複数候補生成（nパラメータ） (優先度: P3)

開発者として、1回のリクエストで複数の生成候補を取得したい。
これにより、ベストな出力の選択、A/Bテスト、多様な応答の提示が可能になる。

**この優先度の理由**: 一部のアプリケーションで使用される機能であるが、
多くのユースケースではn=1で十分なため、優先度を下げている。

**独立テスト**: n=3を指定したリクエストを送信し、choices配列に3つの
異なる生成結果が含まれることを確認できる。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、**実行** n=3でChat Completionsリクエスト、
   **結果** choices配列に3つの要素が含まれる

2. **前提** モデルがロードされている、**実行** n=3でリクエスト、
   **結果** 各choiceのindexが0, 1, 2と順番に設定されている

3. **前提** なし、**実行** n=10（上限超過）でリクエスト、
   **結果** 適切なエラーメッセージまたは上限値での処理

---

### ユーザーストーリー6 - Open Responses API基本リクエスト (優先度: P1)

開発者として、Open Responses API（/v1/responses）を使用してLLMにアクセスしたい。
これにより、業界標準のResponses APIを使用しながら、llmlbの負荷分散機能を
活用できる。

**この優先度の理由**: Responses APIの基本的なパススルー機能がなければ、
この機能全体が成立しない。Ollama v0.13.3+、vLLM、OpenRouter等が対応済み。

**独立テスト**: Responses API対応バックエンドに対して/v1/responsesエンドポイントに
リクエストを送信し、正しいレスポンスが返ることで完全にテストできる。

**受け入れシナリオ**:

1. **前提** Responses API対応バックエンドが登録されている、
   **実行** /v1/responsesにリクエストを送信する、
   **結果** バックエンドからのレスポンスがそのまま返される

2. **前提** 認証済みAPIキーを持っている、
   **実行** /v1/responsesにリクエストを送信する、
   **結果** 既存のAPIキー認証で問題なくアクセスできる

3. **前提** リクエストにモデル名が指定されている、
   **実行** /v1/responsesにリクエストを送信する、
   **結果** 指定モデルを提供するバックエンドにルーティングされる

---

### ユーザーストーリー7 - Open Responses APIストリーミング (優先度: P1)

開発者として、Responses APIのストリーミング機能を使用して、リアルタイムに
トークンを受け取りたい。これにより、ユーザーは応答を待つことなく、
生成されたテキストを即座に確認できる。

**この優先度の理由**: ストリーミングは現代のLLMアプリケーションで必須の機能。
ユーザー体験に直結する。

**独立テスト**: stream=trueを指定したリクエストを送信し、Server-Sent Eventsとして
トークンが順次返されることで完全にテストできる。

**受け入れシナリオ**:

1. **前提** stream=trueを指定している、
   **実行** /v1/responsesにリクエストを送信する、
   **結果** バックエンドからのストリーミングイベントがそのまま転送される

2. **前提** ストリーミング中にエラーが発生した、
   **実行** バックエンドがエラーイベントを送信する、
   **結果** エラーイベントもそのままクライアントに転送される

---

### ユーザーストーリー8 - Responses API非対応バックエンドの処理 (優先度: P1)

開発者として、Responses API非対応のバックエンドへのリクエストがあった場合、
明確なエラーメッセージを受け取りたい。これにより、問題の原因を素早く特定できる。

**この優先度の理由**: エラー処理は基本機能の一部。予期しない動作を防ぎ、
デバッグを容易にする。

**独立テスト**: Responses API非対応バックエンドに対してリクエストを送信し、
501エラーが返されることで完全にテストできる。

**受け入れシナリオ**:

1. **前提** 指定モデルがResponses API非対応バックエンドでのみ利用可能、
   **実行** /v1/responsesにリクエストを送信する、
   **結果** 501 Not Implementedエラーが返される

2. **前提** すべてのバックエンドがResponses API非対応、
   **実行** /v1/responsesにリクエストを送信する、
   **結果** 501 Not Implementedエラーが返される

---

### ユーザーストーリー9 - バックエンド対応状況の確認 (優先度: P2)

開発者として、どのモデルがResponses APIに対応しているかを事前に確認したい。
これにより、適切なAPIエンドポイントを選択できる。

**この優先度の理由**: P1機能がなくても動作するが、クライアントの利便性を
大幅に向上させる。

**独立テスト**: /v1/modelsエンドポイントを呼び出し、各モデルのAPI対応情報が
含まれていることで完全にテストできる。

**受け入れシナリオ**:

1. **前提** Responses API対応バックエンドが登録されている、
   **実行** /v1/modelsを呼び出す、
   **結果** 対応モデルにResponses API対応フラグが含まれる

2. **前提** Chat Completions APIのみ対応のバックエンドがある、
   **実行** /v1/modelsを呼び出す、
   **結果** 該当モデルにはChat Completions APIのみ対応と表示される

---

### ユーザーストーリー10 - バックエンド対応状況の自動検出 (優先度: P2)

システム管理者として、バックエンドのResponses API対応状況が自動的に
検出されてほしい。これにより、手動設定の手間を省ける。

**この優先度の理由**: 手動設定でも運用可能だが、自動検出により運用負荷を軽減できる。

**独立テスト**: 新しいバックエンドを登録し、ヘルスチェック後に対応API情報が
自動的に設定されることで完全にテストできる。

**受け入れシナリオ**:

1. **前提** Responses API対応バックエンドを新規登録した、
   **実行** ヘルスチェックが実行される、
   **結果** 対応API情報が自動的に検出・設定される

2. **前提** Chat Completions APIのみ対応のバックエンドを登録した、
   **実行** ヘルスチェックが実行される、
   **結果** Responses API非対応として記録される

---

### エッジケース

- トークナイザーがない状態でusage計算を要求された場合、何が起こるか?
- 非常に長い入力（コンテキスト上限付近）でのusage計算の精度はどうか?
- ストリーミングモードでlogprobs=trueを指定した場合、どのように処理されるか?
- n > 1とストリーミングを同時に指定した場合、どのように処理されるか?
- presence_penaltyとfrequency_penaltyを同時に高い値に設定した場合の動作は?
- ストリーミング中にバックエンドとの接続が切断された場合、クライアントに接続エラーが通知される
- リクエストにprevious_response_idが含まれている場合、そのままバックエンドに転送される（ステート管理はバックエンドの責務）
- 複数のバックエンドが同じモデルを提供し、一部のみResponses API対応の場合、対応バックエンドが優先される

## 要件 *(必須)*

### 機能要件

- **FR-001**: システムはChat Completionsレスポンスにusageフィールド
  （prompt_tokens, completion_tokens, total_tokens）を含める必要がある
- **FR-002**: システムはCompletionsレスポンスにusageフィールドを含める必要がある
- **FR-003**: システムはトークン数を実際のトークナイザーを使用して計算する必要がある
- **FR-004**: システムは各レスポンスに一意のIDを生成する必要がある
- **FR-005**: システムはcreatedフィールドに現在のUNIXタイムスタンプを設定する必要がある
- **FR-006**: システムはpresence_penaltyパラメータ（-2.0〜2.0）を受け付ける必要がある
- **FR-007**: システムはfrequency_penaltyパラメータ（-2.0〜2.0）を受け付ける必要がある
- **FR-008**: システムはlogprobs=trueで実際の確率値を返す必要がある
- **FR-009**: システムはtop_logprobsパラメータ（1〜20）を受け付ける必要がある
- **FR-010**: システムはnパラメータ（1〜上限値）で複数候補を生成する必要がある
- **FR-011**: システムは範囲外のパラメータに対して適切なエラーを返す必要がある

#### Open Responses API関連（追加）

- **FR-012**: システムは/v1/responsesエンドポイントを提供する必要がある
- **FR-013**: システムはResponses APIリクエストをバックエンドにそのままパススルーする必要がある
- **FR-014**: システムはバックエンドからのレスポンス（エラー含む）をそのままクライアントに返す必要がある
- **FR-015**: システムはストリーミングイベントをそのまま転送する必要がある
- **FR-016**: システムはResponses API非対応バックエンドへのリクエストに対して501 Not Implementedを返す必要がある
- **FR-017**: システムは既存のAPIキー認証を/v1/responsesエンドポイントにも適用する必要がある
- **FR-018**: システムは/v1/modelsレスポンスに各モデルの対応API情報を含める必要がある
- **FR-019**: システムはヘルスチェック時にバックエンドのResponses API対応状況を検出する必要がある

### 主要エンティティ

- **TokenUsage**: プロンプトと生成のトークン数を表す。prompt_tokens、
  completion_tokens、total_tokensの3つの値を持つ
- **ResponseId**: 各APIレスポンスを一意に識別するID。プレフィックス（chatcmpl-/cmpl-）と
  一意の識別子で構成される
- **LogprobInfo**: トークンの確率情報。トークン文字列、log確率、
  上位候補トークンのリストを含む
- **Endpoint**: LLMバックエンドの接続情報と対応API情報を含む（Open Responses API追加）
- **SupportedAPIs**: モデルが対応するAPI一覧（chat_completions, responses等）

---

## 関連仕様

- [SPEC-82cd11b7](../SPEC-82cd11b7/spec.md): API統合リファレンス — 全APIエンドポイントのカタログ・認証モデル分類・設計規約

## スコープ外 *(オプション)*

以下の機能は本仕様のスコープ外とし、将来のバージョンで対応予定:

- Assistants API、Threads API等の高レベルAPI
- Fine-tuning API
- Files API
- Moderations API
- visionのdetailパラメータ
- Chat Completions APIとResponses API間の変換（パススルーのみ）
- ステートフル機能（previous_response_id）の管理（バックエンドの責務）
- ツール実行（function_call、web_search等）の処理（クライアントの責務）
- xLLM側のResponses API実装（別SPECで対応）

---

## 技術制約 *(該当する場合)*

- llama.cppのトークナイザー機能に依存する
- logprobs計算にはllama.cppのlogits取得機能が必要
- n > 1の生成は推論時間が線形に増加する
- バックエンドがResponses APIに対応していない場合、変換は行わない（パススルーのみ）
- ストリーミングイベントの形式は変更しない（完全パススルー）

---

## 前提条件 *(該当する場合)*

この機能は以下を前提とします:

- llama.cppがトークナイザー機能を提供していること
- llama.cppがサンプリング時のlogits情報を取得可能であること
- 現在のOpenAI互換APIエンドポイント（/v1/chat/completions等）が動作していること
- 少なくとも1つのResponses API対応バックエンド（Ollama v0.13.3+、vLLM、xLLM等）が存在する（Open Responses API用）
- クライアントはResponses API仕様を理解している（Open Responses API用）
- 既存のAPIキー認証システムが動作している

---

## 依存関係 *(該当する場合)*

この機能は以下に依存します:

- llama.cpp（トークナイザー、サンプリング、logits取得）
- 既存のOpenAI互換APIエンドポイント実装
- 既存のバックエンドルーティングシステム（Open Responses API用）
- 既存のAPIキー認証システム（Open Responses API用）
- 既存のヘルスチェックシステム（Open Responses API用）

---

## 成功基準 *(必須)*

以下の成功基準を満たす必要があります:

1. OpenAI公式SDKを使用したアプリケーションがエラーなく動作する
2. usageフィールドのトークン数が実際の値と±10%以内の精度である
3. 各レスポンスIDが一意であり、100万リクエストで重複がない
4. logprobsの値が実際の確率を反映し、ダミー値（0.0）ではない
5. presence_penalty、frequency_penaltyが生成結果に影響を与える
6. n > 1でリクエスト時、指定した数の候補が返される
7. 既存のOpenAI互換テスト（make openai-tests）が引き続きパスする
8. Responses API対応バックエンドへのリクエストが正常に処理され、レスポンスが返される
9. ストリーミングリクエストでトークンが遅延なく順次返される
10. 非対応バックエンドへのリクエストが501エラーで明確に拒否される
11. /v1/modelsでモデルごとの対応API情報が確認できる
12. 新規バックエンド登録後、対応API情報が自動的に検出される
