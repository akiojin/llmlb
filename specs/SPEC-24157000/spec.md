# 機能仕様書: OpenAI互換API完全準拠

**機能ID**: `SPEC-24157000`
**作成日**: 2026-01-05
**ステータス**: 実装中（Phase 3.6: logprobs実値化）
**入力**: ユーザー説明: "OpenAI互換API完全準拠 - 内蔵エンジンのOpenAI互換APIを100%準拠にする"

## ユーザーシナリオ＆テスト *(必須)*

### ユーザーストーリー1 - トークン使用量の正確な取得 (優先度: P1)

開発者として、APIレスポンスから正確なトークン使用量を取得したい。
これにより、アプリケーションのコスト計算、使用量監視、クォータ管理を
正確に行うことができる。

**この優先度の理由**: トークン課金や使用量制限を行うアプリケーションにとって
最も重要な情報であり、多くのOpenAI SDKがこの値を前提としている。

**独立テスト**: Chat CompletionsまたはCompletionsエンドポイントを呼び出し、
レスポンスのusageフィールドにprompt_tokens、completion_tokens、total_tokensが
正確な値で含まれていることを確認できる。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、**実行** Chat Completionsリクエストを送信、
   **結果** レスポンスにusage.prompt_tokens、usage.completion_tokens、
   usage.total_tokensが含まれ、total_tokens = prompt_tokens + completion_tokensである

2. **前提** 既知の入力トークン数のプロンプトを用意、**実行** リクエストを送信、
   **結果** 返されたprompt_tokensが実際のトークン数と一致する（±5%以内）

3. **前提** ストリーミングモードでリクエスト、**実行** レスポンスを受信、
   **結果** 最終チャンクまたは別途usageが取得可能

---

### ユーザーストーリー2 - レスポンスIDとタイムスタンプの一意性 (優先度: P1)

開発者として、各APIレスポンスに一意のIDと正確なタイムスタンプを取得したい。
これにより、リクエストのトレーシング、ログ分析、デバッグを効率的に行うことができる。

**この優先度の理由**: 本番環境でのトラブルシューティングやログ集約において、
各レスポンスを一意に識別できることは運用上必須である。

**独立テスト**: 連続して複数のリクエストを送信し、各レスポンスのidフィールドが
異なる値であること、createdフィールドが現在時刻に近いことを確認できる。

**受け入れシナリオ**:

1. **前提** なし、**実行** 2つのChat Completionsリクエストを連続送信、
   **結果** 各レスポンスのidフィールドが異なる一意の値である

2. **前提** なし、**実行** リクエストを送信、
   **結果** createdフィールドが現在のUNIXタイムスタンプ（±5秒以内）である

3. **前提** なし、**実行** リクエストを送信、
   **結果** idフィールドが「chatcmpl-」または「cmpl-」プレフィックスで始まる

---

### ユーザーストーリー3 - 繰り返しペナルティパラメータの使用 (優先度: P2)

開発者として、presence_penaltyとfrequency_penaltyパラメータを使用して、
生成テキストの多様性を制御したい。
これにより、OpenAI APIと同じ方法で出力の繰り返しを抑制できる。

**この優先度の理由**: 多くのプロンプトエンジニアリング手法やアプリケーションが
これらのパラメータを活用しており、互換性のために重要である。

**独立テスト**: presence_penaltyやfrequency_penaltyを設定したリクエストが
エラーなく処理され、出力に影響を与えることを確認できる。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、**実行** presence_penalty=1.0でリクエスト、
   **結果** リクエストがエラーなく処理され、レスポンスが返される

2. **前提** モデルがロードされている、**実行** frequency_penalty=1.0でリクエスト、
   **結果** リクエストがエラーなく処理され、レスポンスが返される

3. **前提** なし、**実行** presence_penalty=3.0（範囲外）でリクエスト、
   **結果** 適切なエラーメッセージで400エラーが返される

---

### ユーザーストーリー4 - トークン確率（logprobs）の取得 (優先度: P2)

開発者として、生成された各トークンの確率情報を取得したい。
これにより、モデルの信頼度分析、出力品質の評価、トークン選択の
詳細な分析が可能になる。

**この優先度の理由**: 研究用途やモデル評価において重要な機能であり、
OpenAI APIの主要な機能の一つである。

**独立テスト**: logprobs=trueを指定したリクエストを送信し、各トークンの
確率情報が実際の値で返されることを確認できる。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、**実行** logprobs=trueでChat Completions、
   **結果** choices[0].logprobs.contentに各トークンの確率情報が含まれる

2. **前提** モデルがロードされている、**実行** top_logprobs=5でリクエスト、
   **結果** 各トークン位置で上位5つの候補トークンとその確率が返される

3. **前提** なし、**実行** logprobs情報を確認、
   **結果** logprobの値が負の実数（log確率）であり、0.0ではない

4. **前提** モデルがロードされている、**実行** logprobs=trueでリクエスト、
   **結果** logprobの値がllama_get_logits()から取得した実際のlogits値に基づいている
   - **実装注記（Phase 3.6）**: 初期実装ではハッシュベースの疑似値を使用。
     実値化によりモデルの実際の確信度を反映した値に置き換える。

---

### ユーザーストーリー5 - 複数候補生成（nパラメータ） (優先度: P3)

開発者として、1回のリクエストで複数の生成候補を取得したい。
これにより、ベストな出力の選択、A/Bテスト、多様な応答の提示が可能になる。

**この優先度の理由**: 一部のアプリケーションで使用される機能であるが、
多くのユースケースではn=1で十分なため、優先度を下げている。

**独立テスト**: n=3を指定したリクエストを送信し、choices配列に3つの
異なる生成結果が含まれることを確認できる。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、**実行** n=3でChat Completionsリクエスト、
   **結果** choices配列に3つの要素が含まれる

2. **前提** モデルがロードされている、**実行** n=3でリクエスト、
   **結果** 各choiceのindexが0, 1, 2と順番に設定されている

3. **前提** なし、**実行** n=10（上限超過）でリクエスト、
   **結果** 適切なエラーメッセージまたは上限値での処理

---

### エッジケース

- トークナイザーがない状態でusage計算を要求された場合、何が起こるか?
- 非常に長い入力（コンテキスト上限付近）でのusage計算の精度はどうか?
- ストリーミングモードでlogprobs=trueを指定した場合、どのように処理されるか?
- n > 1とストリーミングを同時に指定した場合、どのように処理されるか?
- presence_penaltyとfrequency_penaltyを同時に高い値に設定した場合の動作は?

## 要件 *(必須)*

### 機能要件

- **FR-001**: システムはChat Completionsレスポンスにusageフィールド
  （prompt_tokens, completion_tokens, total_tokens）を含める必要がある
- **FR-002**: システムはCompletionsレスポンスにusageフィールドを含める必要がある
- **FR-003**: システムはトークン数を実際のトークナイザーを使用して計算する必要がある
- **FR-004**: システムは各レスポンスに一意のIDを生成する必要がある
- **FR-005**: システムはcreatedフィールドに現在のUNIXタイムスタンプを設定する必要がある
- **FR-006**: システムはpresence_penaltyパラメータ（-2.0〜2.0）を受け付ける必要がある
- **FR-007**: システムはfrequency_penaltyパラメータ（-2.0〜2.0）を受け付ける必要がある
- **FR-008**: システムはlogprobs=trueで実際の確率値を返す必要がある
- **FR-009**: システムはtop_logprobsパラメータ（1〜20）を受け付ける必要がある
- **FR-010**: システムはnパラメータ（1〜上限値）で複数候補を生成する必要がある
- **FR-011**: システムは範囲外のパラメータに対して適切なエラーを返す必要がある

### 主要エンティティ

- **TokenUsage**: プロンプトと生成のトークン数を表す。prompt_tokens、
  completion_tokens、total_tokensの3つの値を持つ
- **ResponseId**: 各APIレスポンスを一意に識別するID。プレフィックス（chatcmpl-/cmpl-）と
  一意の識別子で構成される
- **LogprobInfo**: トークンの確率情報。トークン文字列、log確率、
  上位候補トークンのリストを含む

---

## スコープ外 *(オプション)*

以下の機能は本仕様のスコープ外とし、将来のバージョンで対応予定:

- Assistants API、Threads API等の高レベルAPI
- Fine-tuning API
- Files API
- Moderations API
- visionのdetailパラメータ

---

## 技術制約 *(該当する場合)*

- llama.cppのトークナイザー機能に依存する
- logprobs計算にはllama.cppのlogits取得機能が必要
- n > 1の生成は推論時間が線形に増加する

---

## 前提条件 *(該当する場合)*

この機能は以下を前提とします:

- llama.cppがトークナイザー機能を提供していること
- llama.cppがサンプリング時のlogits情報を取得可能であること
- 現在のOpenAI互換APIエンドポイント（/v1/chat/completions等）が動作していること

---

## 依存関係 *(該当する場合)*

この機能は以下に依存します:

- llama.cpp（トークナイザー、サンプリング、logits取得）
- 既存のOpenAI互換APIエンドポイント実装

---

## 成功基準 *(必須)*

以下の成功基準を満たす必要があります:

1. OpenAI公式SDKを使用したアプリケーションがエラーなく動作する
2. usageフィールドのトークン数が実際の値と±10%以内の精度である
3. 各レスポンスIDが一意であり、100万リクエストで重複がない
4. logprobsの値が実際の確率を反映し、ダミー値（0.0）ではない
5. presence_penalty、frequency_penaltyが生成結果に影響を与える
6. n > 1でリクエスト時、指定した数の候補が返される
7. 既存のOpenAI互換テスト（make openai-tests）が引き続きパスする
