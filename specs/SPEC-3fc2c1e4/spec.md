# SPEC-3fc2c1e4: 実行エンジンE統合仕様！E

**スチEEタス**: 計画中E統合！E

## こE仕様E役割EE見向け！E
こE仕様E**実行エンジン領域の入口ガイチE*です、E 
エンジン選抁E抽象化E原則をまとめ、詳細は下記E個別SPECに委譲します、E

## 背景 / 問顁E
推論エンジンに関する要件がE散し、モチE管琁E混在することで責務が曖昧になってぁE、E
実行エンジン領域を統合し、モチE管琁Eは明確に刁Eする忁Eがある、E

## 目皁E
- Node側エンジン抽象化と推論責務を統合的に定義する
- GPU前提EEetal/CUDAEE実行要件をE確化すめE
- エンジン選択が登録時EアーチEファクトに従うことを保証する

## スコーチE
- Node側のエンジン抽象化！EngineRegistry/Engineの責務！E
- 実行環墁EE前提EEPU忁E！E
- マニフェストとローカル実体に基づくエンジン選抁E

## 非ゴール
- モチE登録・保存（モチE管琁E域EE
- 自動変換/量子化生E
- Nemotron推論エンジンの詳細設計！EBDEE

## 原則
- `metadata.json` のような独自メタチEEタには依存しなぁE
- エンジン選択E「Eニフェストとローカル実体」を正とする
- 形式選択ERouterで行わず、Nodeがruntime/GPU要件に応じて判断する

## 決定事頁EE有用サマリEE
- **責務E離**: 形式選択ENode側で行い、ルーターはマニフェスト提供に徹する、E
- **Node前提**: Node は Python 依存を導EしなぁEE
- **GPU前提**: GPU 非搭載ノードE対象外（登録不可E、E
- **対応OS/GPU**:
  - macOS: Apple SiliconEEetalEE
  - Windows: CUDA
  - Linux: 当面は非対忁E
  - WSL2: 対象外！EindowsはネイチEブEみEE
- **CUDAoH̗R**: WindowsłCUDA萫/ČŗDʁADirectML̓A[eBt@NgsƃhCỏe傫ߓiĝ݁jB
- **形式選抁E*: safetensors/GGUF 等E **format は登録時に確宁E*し、実行時の刁Eは行わなぁEE 
  NodeはGPUに応じて **公式最適化アーチEファクチE*E侁E Metal/CUDA向けEを選択する、E
- **最適化アーチEファクチE*: 公式最適化アーチEファクトE利用優先Eエンジン領域の実行最適化として扱ぁENodeが選択したアーチEファクトを置き換えなぁEE
- **Nemotron**: 新エンジンの仕槁E実裁EE後回し！EBDE、E
- **冁Eエンジンの要件は単一匁E*: 詳細は「E蔵エンジン要件E単一要件E」に統合済み、E

## 冁Eエンジン要件E単一要件EE

- **REQ-IE-001**: 冁Eエンジンは **RuntimeType/format/capabilities** に基づく単一の刁E規紁E持ち、E 
  LLM/EmbeddingEELlamaCpp`,`GptOssCpp`,`NemotronCpp`E、ASREEWhisperCpp`E、TTSEEOnnxRuntime`E、E 
  画像生成！EStableDiffusion`E、画像認識（新エンジンEとして扱ぁEE 
  **冁Eエンジンの具体侁E*は llama.cpp / gpt-oss / nemotron / whisper.cpp / stable-diffusion.cpp / ONNX Runtime とする、E 
  併せて、次の条件めE**1つの要件として**満たすこと:
  - **プラグイン形弁E*: Node 本体E Engine Host とし、エンジンは動的プラグインEEdylib/.so/.dllEで追加可能、E
  - **ABI固宁E*: プラグインは C ABI で互換性を保証し、`abi_version` を忁Eとする、E
  - **選択ソース**: 登録時に確定しぁE`format` と HF 由来メタチEEタEEconfig.json` 等）を正とし、E 
    `metadata.json` のような独自メタチEEタには依存しなぁEE
  - **自動フォールバック禁止**: safetensors/GGUF がE存する場合E登録時E `format` 持Eが忁Eで、E 
    実行時の形式E替は行わなぁEE
  - **GPU前提**: エンジンは GPU 前提EEacOS=Metal / Windows=CUDA / Linux=Cudaは実験扱ぁE、E
  - **可否判宁E*: こE刁EE EngineRegistry/EngineHost および `/v1/models` の可否判定に反映され、E 
    **未対応カチEリは登録対象から除夁E*される、E

## 冁EエンジンのアーキチEチャE概念EE

> 目皁E **冁Eエンジン群の責務墁Eと選択フロー**を一枚で把握できるようにする、E

### 構E図E概念EE

```
┌──────────────━E              ┌────────────────────────────────────━E
━E Router      ━E              ━E               Node                ━E
━E - 登録/メタ │──manifest────▶━E ModelStorage / Resolver            ━E
━E - HF検証    ━E              ━E - config/tokenizer検証             ━E
└──────────────━E              ━E - runtime確宁E                     ━E
                                ━E - 忁EアーチEファクト選抁E        ━E
                                ━E - 外部ソースEEF等）から直接取征E  ━E
                                ━E            ━E
                                ━E            ▼
                                ━E    EngineRegistry
                                ━E (RuntimeTypeで選抁E
                                ━E            ━E
                                ━E            ▼
                                ━E Engine Host (Plugin Loader)
                                ━E   ├─ GGUF ↁEllama.cpp (plugin)
                                ━E   ├─ TTS  ↁEONNX Runtime (plugin)
                                ━E   └─ safetensors ↁE独自エンジン群 (plugins)
                                ━E         ├─ gpt-oss (Metal/CUDA)
                                ━E         ├─ nemotron (TBD)
                                ━E         └─ そE他！Ehisper/SD などEE
                                └────────────────────────────────────━E
```

### 主要コンポEネンチE

- **Router**
  - 登録時に **メタチEEタとマニフェスト（ファイル一覧EE* を保存する、E
  - **形式E登録時に確宁E*し、Nodeはformatを尊重するE実行時の刁Eは行わなぁE、E
  - ルーターはモチEバイナリを保持しなぁEE
- **Node / ModelStorage + Resolver**
  - 形式Eファイルの整合性EEconfig.json` / `tokenizer.json` / shard / indexEを検証、E
  - GPUバックエンドに応じて **忁EアーチEファクトを選抁E*し、外部ソースEEF等）から直接取得する、E
  - `ModelDescriptor` を生成！Eormat / primary_path / runtime / capabilitiesE、E
- **EngineRegistry**
  - `RuntimeType` に基づぁE**外Eの推論エンジンを確宁E*する、E
  - Node はマニフェストとローカル実体を正とし、E*実行時の自動変換は行わなぁE*、E
- **Inference EngineE外EEE*
  - 共通E推論インターフェース。E部で runtime に応じてプラグインを振りEける、E
  - GGUF ↁE`llama.cpp`、TTS ↁE`ONNX Runtime`、safetensors ↁE独自エンジン群EすべてプラグインE、E
  - **GGUFは llama.cpp が褁EアーキチEチャEElama/mistral/gemma/phi 等）を横断皁E駁E**する、E
  - **safetensorsは原則「モチEごとの専用エンジン、E*で対応する（侁E gpt-oss, nemotronE、E
    - 汎用safetensorsエンジンの可能性は否定しなぁE、E*初期要件では前提にしなぁE*、E
  - 公式最適化アーチEファクトE **実行キャチEュ**として利用可能だが、E
    Nodeが選択したアーチEファクトE上書きしなぁEE

### GPUバックエンド（最下層レイヤーEE

- **Metal**EEacOS / Apple SiliconEE
- **CUDA**EEindows / NVIDIAEE
  - Linux: 当面は非対忁E

## 現状の対応済みモチE/アーキチEチャEE026-01-02時点EE

- **GGUF / llama.cpp**: llama/mistral/qwen/gemma/phi 系が検証済み、E 
  詳細なモチEIDと検証状況E `specs/SPEC-6cd7f960/verified-models.md` を正とする、E
- **safetensors / 冁Eエンジン**: 実運用で確認できてぁEのは **gpt-ossEEetal/macOSEE* のみ、E 
  DirectMLは実験扱ぁENemotronはTBDE後回し）、E
- **そE他モダリチE**EESR/TTS/画像生戁E画像認譁EEmbeddingEE  
  対応可否と検証済みモチEは `specs/SPEC-6cd7f960/verified-models.md` で管琁Eる、E

### プラグイン設計指針！EodeEE

- **配币E佁E*: 共有ライブラリ + manifest.json の 1 セチE
- **manifest冁E**:
  - engine_id / engine_version / abi_version
  - 対忁ERuntimeType / 形式！Eafetensors, gguf, onnx 等！E
  - 対忁EcapabilitiesEEext / vision / asr / tts / imageEE
  - GPU 要件EEetal / DirectML / CUDA(実騁EEE
- **互換性**: C ABI を固定し、ABI 互換を破る変更は abi_version を更新する
- **解決頁EE*: EngineRegistry ぁERuntimeType と format をキーにプラグインを解決する
  - ベンチEーク未設定E場合、E*プラグインE非builtinEを優允E*し、builtinはフォールバックとする

### RuntimeType とエンジンの対応（現状EE

| RuntimeType | 主用送E| 主要アーチEファクチE| 備老E|
|---|---|---|---|
| `LlamaCpp` | LLM / Embedding | GGUF | NodeがGGUFアーチEファクトを選択した場吁E|
| `GptOssCpp` | gpt-oss | safetensors + 公式最適匁E| macOSはMetal最適化、WindowsはCUDAを主経路 |
| `NemotronCpp` | Nemotron | safetensors | **TBD**EEindows CUDA想定、Linux CUDAは実験扱ぁEE|
| `WhisperCpp` | ASR | GGML/GGUFE当面EE| 変換は行わなぁEsafetensors対応E封E検訁E|
| `StableDiffusion` | 画像生戁E| safetensorsE直接EE| stable-diffusion.cpp を当面利用 |
| `OnnxRuntime` | TTS | ONNX | Python依存なしで運用する |

**現状の実運用確誁E*
- safetensors系LLMで安定動作が確認できてぁEのは **gpt-ossEEetal/macOSEE* のみ、E
- Windows CUDAが主経路、DirectMLは限定的、NemotronはTBDE後回し）、E

### 現在の対応済みモチEEE026-01-02時点EE

**Model HubEErouter/src/supported_models.json`Eに登録済み**の篁E:

- **GGUF / llama.cpp**:
  - Qwen2.5 7B Instruct
  - Llama 3.2 3B Instruct
  - Mistral 7B Instruct
  - Phi-3 Mini
  - Gemma 2 9B
- **safetensors / gpt-oss**:
  - GPT-OSS 20BEEetalEE
  - GPT-OSS 120BEEetalEE
  - GPT-OSS Safeguard は **Metal最適化アーチEファクト未提侁E*のため未対忁E

詳細な検証状況E `specs/SPEC-6cd7f960/verified-models.md` を参照、E

### アーチEファクト選択とエンジン選択E原則

1. **Router は形式を確定せぁE*、EニフェストEみを提供する、E
2. **Node がruntime/GPU要件に応じてアーチEファクトを選抁E*する、E
3. **変換は行わなぁE*EEafetensors/GGUF/MetalはそEまま扱ぁE、E
4. **最適化アーチEファクトE “実行キャチEュ E* として利用可能だが、E
   ローカル実体に存在しなぁE合EHFから直接取得する、E

## 性能/メモリ要件E測定と制紁EE

- **測定タイミング**: モチE登録時Eエンジン更新時にベンチEークを実行する、E
- **測定指樁E*: throughputEEokens/secE、TTFT、VRAM使用玁Eピーク/平坁Eを記録する、E
- **測定条件**: コンチEスト長めEチEサイズなどの条件をメタチEEタとして保持する、E
- **制紁E*: VRAM使用玁E90%を趁E、またEOOMの場合E失敗として扱ぁEE
- **反映**: 測定結果は EngineRegistry の選択に反映する、E

### Nemotron の位置づぁE

- 冁Eエンジンの **一部として Nemotron 対応を含む**、E
- **Windows CUDA を想宁E*し、Linux CUDA は実験扱ぁEEetalは封E対応）、E
- Nemotron 専用の詳細設計E **TBD** として後段 SPEC に委譲、E
- **Windows CUDA (TBD)**: gptoss_* C API ??????DLL/ENV ? TBD?
- **DirectML (??)**: ??????????????`model.directml.bin` / `model.dml.bin` ??????

## 詳細仕様（参照EE
- **エンジン抽象匁E*: `SPEC-d7feaa2c`
- **gpt-oss-20b safetensors 実衁E*: `SPEC-2c0e5a9b`
- **gptossアーキチEチャエイリアス**: `SPEC-8a2d1d43`
- **Nemotron PoC**: `SPEC-efff1da7`

## 受け入れ条件
1. Nodeのエンジン選択E登録済みアーチEファクトとHFメタチEEタに一致する、E
2. GPU非搭載ノードE対象外とする、E
3. モチE管琁EE仕様と矛盾しなぁEE

## 依存関俁E
- `SPEC-08d2b908`EモチE管琁E合！E
- `SPEC-5cd7b614`EEPU忁Eノード登録要件EE

---

## Clarifications

### Session 2025-12-30

インタビューにより以下が確宁E

**アーキチEチャ基盤**:

- **プロセス刁E**: 同一プロセスEモノリシチEEE
  - Engine HostとEngineプラグインは同一プロセス空間で動佁E
- **ビルド方弁E*: 動的リンクEEHARED ライブラリEE
  - 吁EンジンEElama.cpp、gptoss等）E.so/.dylib/.dllとして個別ビルチE
  - `EngineHost`が`dlopen`/`LoadLibraryA`で動的ローチE
- **コンチEスト設宁E*: モチE定義に固定値
  - supported_models.json等でモチEごとにコンチEスト長を定義

**エンジン選択戦略**:

- **ホットスワチEE**: 不要EE起動で刁EEE
  - ノEドE起動時のみエンジン構Eを変更可能
- **優先頁E決宁E*: 性能ベンチEーク自動選抁E
  - 同一モチEに褁Eエンジンが対応可能な場合、EンチEーク結果で決宁E
- **VRAMアロケータ**: エンジン任せ！ES/ドライバ依存！E
  - 吁Eンジンが独自にメモリ管琁E競合時はOOMエラー
- **サードパーチEプラグイン**: サポEトする（制限なし！E
  - 任意Eプラグインをディレクトリ配置でロード可能

**ベンチEーク仕槁E*:

- **実行タイミング**: モチE登録晁E
  - モチEをsupported_modelsに登録する際にベンチEーク実衁E
- **測定指樁E*: 褁Eスコア
  - スループット！Eokens/secEE TTFT + VRAM使用玁EE加重スコア
- **結果保孁E*: モチEメタチEEタに埋め込み
  - supported_models.json等にベンチEーク結果を追訁E
- **保存形式（暫定！E*:
  - モチEメタチEEタ冁E「engine_id ↁE褁Eスコア」E対応を保持する
  - EngineRegistryはこEスコアを参照して同一runtime冁EEエンジンを選択すめE
  - 参EできなぁE合E登録頁EE先頭エンジンを選択し、警告ログを残す

**エラーハンドリング**:

- **クラチEュ対忁E*: エンジン再ローチE
  - SEGFAULT等でクラチEュしたエンジンのみをEロード、他E維持E
- **機E未対忁E*: エラー返却EE01 Not ImplementedEE
  - エンジンがサポEトしなぁEEEEision、function calling等）Eエラー返却

**チEEタ管琁E*:

- **中間データエクスポEチE*: サポEトしなぁE
  - アチEション重み、アクチEベEション等E冁E状態E非E閁E
- **KVキャチEュ共朁E*: 不要E
  - エンジン刁E時EコンチEストリセチE、単純化を優允E

**プラグイン設計詳細**:

- **インストEル方況E*: チEレクトリ配置のみ
  - プラグインチEレクトリに.so/.dylib + manifest.jsonをE置
- **ABI不一致**: ロード拒否 + アラーチE
  - ダチEュボEドやログで非互換プラグインを通知
- **カスタムパラメータ**: manifest.jsonに全て定義
  - エンジン固有設定Emanifestに記述
- **モダリチE統一**: 統一する
  - チEスチE画僁E音声の全モダリチEで同一Engineインターフェース
  - 入出力EモダリチE別メソチEEEenerate_text(), generate_image()等）で刁E

**ライフサイクル管琁E*:

- **アンロードE琁E*: destroy + dlclose
  - エンジン破棁EにライブラリもアンローチE
- **ログ統吁E*: 標準E力キャプチャ
  - stdout/stderrをノードがキャプチャして統合ログへ
- **ヘルスチェチE**: 不要E
  - プロアクチEブな確認なし、エラー時Eみ検E

**セキュリチE・制陁E*:

- **タイムアウチE*: 不要E
  - エンジンを信頼、デチEロチE検EなぁE
- **ID競吁E*: ロードエラー
  - 同一engine_idの2つ目以降Eロード拒否
- **メモリ制陁E*: 制限なぁE
  - エンジンは任意にRAM/VRAMを使用可能
- **ネットワーク**: 禁止
  - エンジンからの外部ネットワークアクセスEモチEダウンロード等）E禁止

### Session 2025-12-30 (詳細インタビュー)

追加インタビューにより以下が確宁E

**障害検知と復旧**:

- **クラチEュ時EリクエスチE*: 即座に500エラーを返却
  - クラチEュ時点で処琁Eのリクエストには500を返し、クライアントにリトライ判断を委EめE
- **メモリリーク/VRAM枯渁E*: 閾値趁Eで自動E起勁E
  - VRAM使用玁E90%を趁Eた場合、エンジンをE動的に再起勁E
- **ハング検知**: ウォチEドッグスレチE
  - 別スレチEからエンジンの応答性を監視、E0秒タイムアウトでハングと判宁E
- **ハング時E対処**: ノEドE体を再起勁E
  - 同一プロセスのためエンジンのみの強制終亁EE不可、ノードEロセス全体を再起勁E
- **VRAM OOM晁E*: エラー返却と継綁E
  - 該当リクエストにエラーを返し、エンジンは稼働継綁E

**バEジョニングと互換性**:

- **更新検知**: manifest.jsonのバEジョン番号で検知
  - ノEドE起動時にチEレクトリをスキャン、バージョン変更を検E
- **ベンチEーク再実衁E*: エンジン更新時に自動E実衁E
  - manifest.json更新検知時に関連モチEの全ベンチEークをE動E実衁E
- **ベンチEーク実行タイミング**: リリース前E手動チEチE
  - ランタイムではなくCI/CDまたEリリース前に実行し、結果をsupported_models.jsonに埋め込み
- **バEジョン共孁E*: 禁止
  - 同一engine_idの褁EバEジョンはロードエラー
- **ABIバEジョン形弁E*: semver
  - "1.0.0"形式EセマンチEチEバEジョニング

**チEチEとトラブルシューチEング**:

- **エラーメチEージ形弁E*: パススルー
  - エンジン固有EエラーメチEージをそのままクライアントに伝達
- **基本エラーコーチE*: 共通定義
  - OK/ERROR/OOM/TIMEOUTなど基本コードEみ共通、詳細はエンジン固朁E
- **ログレベル設宁E*: manifest.jsonに固宁E
  - 吁EンジンのチEォルトログレベルをmanifestで定義
- **エンジン惁EAPI**: フルダンチE
  - manifest.jsonの冁EをほぼそEまま返却
- **エラー通知**: stdout/stderrをキャプチャ
  - ログ経由でエラーを検E、リアルタイム通知は行わなぁE
- **ロード失敗通知**: ログのみ
  - GPUメモリ不足、依存ライブラリ不足等Eログに記録、E動的な通知なぁE

**実裁E細**:

- **トEクンチEーチE*: エンジンがデコード済み斁EEを返す
  - エンジン冁Eでtokenizerを保持し、デコード済みチEストを返却
- **並行リクエスチE*: Engine Hostがキューイング
  - Hostがリクエストをキューに入れ、EEエンジンに投E
- **ストリーミング**: コールバック関数
  - エンジンがトークンごとにHost提供Eコールバックを呼ぶ
- **キャンセル方況E*: コールバックの戻り値
  - ストリーミングコールバックがfalseを返すとキャンセル
- **GPU別バイナリ**: GPU別に別バイナリ
  - llama_cpp_metal.dylib, llama_cpp_directml.dllのように刁E
- **初期化コンチEスチE*: 詳細惁E
  - チEイスID、VRAMサイズ、ドライババージョン、対応機E等を提侁E
- **エンジンローチE*: 遁EローチE
  - 最初Eリクエスト時に忁EなエンジンをローチE
- **アンロード頁EE*: LRU
  - 最も最近使われてぁEぁEンジンからアンローチE

**エチEケース処琁E*:

- **形式不一致検知**: モチE登録晁E
  - 登録時にフォーマットとエンジンの対応を検証、不一致は拒否
- **GPU競合時**: 即座にエラーEE29 Too Many RequestsEE
  - LLMがGPUを占有中に別モダリチEのリクエストが来た場吁E
- **クライアントE断**: 即座に中断
  - 推論を即時停止、E刁E果は返却しなぁE
- **max_tokens制陁E*: エンジンがE琁E
  - max_tokensをエンジンに渡し、エンジンが停止判宁E
- **STOPトEクン**: HostがE琁E
  - HostがE力を監視し、STOP検知時にエンジンに停止持E
- **usage形弁E*: エンジンがOpenAI形式で返す
  - {prompt_tokens, completion_tokens}形式で返却
- **vision入劁E*: 生バイトE刁E
  - チEード済みのピクセルチEEタをエンジンに渡ぁE
- **Function Calling**: C構造体にマッチE
  - チEEル定義をABI定義の構造体にマッピング

**開発老E騁E*:

- **SDK提侁E*: ヘッダー + チEプレーチE
  - Cヘッダーファイルと最小限のスケルトンプロジェクトを提侁E
- **チEト方況E*: 提供しなぁE
  - サードパーチEが独自にチEト環墁E構篁E
- **署名検証**: なぁE
  - 署名検証なしでロード、ユーザーの自己責任
- **品質保証**: ユーザーの自己責任
  - サードパーチEプラグインの品質はインストEルしたユーザーが責任を持つ

**C ABIインターフェース**:

- **忁EエクスポEト関数**: フルセチE
  - create/destroy/infer + load_model/unload_model/get_info + cancel/get_metrics/set_config
- **未対応モダリチE**: NOT_SUPPORTEDを返すスタチE
  - 全関数をエクスポEトし、未対応Eエラーコードを返す
- **get_info()冁E**: フル惁E
  - engine_id, version, capabilities, gpu_backends, ロード済みモチE一覧、メモリ使用量筁E

**チEレクトリ構造**:

- **プラグイン配置**: GPU別サブディレクトリ
  - `engines/llama_cpp/metal/llama_cpp.dylib`
  - `engines/llama_cpp/directml/llama_cpp.dll`
  - `engines/llama_cpp/manifest.json`EE通！E

**manifest.json拡張**:

- **依存ライブラリ**: ライブラリ吁E+ バEジョン
  - 忁Eな外部ライブラリの名前と最小バージョンをE訁E
- **GPU表現**: 褁E対応可能
  - `"gpu_backends": ["metal", "directml"]`のように配Eで表現
- **モチEパス**: 相対パスEモチEチEレクトリ基準！E
  - エンジンにはモチEチEレクトリからの相対パスを渡ぁE
- **エンコーチEング**: UTF-8固宁E
  - 全ての斁EE入出力EUTF-8を前揁E

**トレードオフと懸念事頁E*:

- **クラチEュリスク**: 封E皁Eプロセス刁Eも検訁E
  - 同一プロセスによる道連れリスクは許容、安定性問題が出れE刁Eを検訁E
- **最大の懸念**: パフォーマンス
  - ABIオーバEヘッドやIPC相当Eコストがボトルネックになる可能性
