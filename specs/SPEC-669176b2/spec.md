# 機能仕様書: llmlb CLIコマンド

**機能ID**: `SPEC-669176b2`
**作成日**: 2026-01-08
**ステータス**: 実装完了
**入力**: ollama互換CLIコマンドの実装

## ユーザーシナリオ＆テスト *(必須)*

### ユーザーストーリー1 - モデルのダウンロードと実行 (優先度: P1)

ユーザーとして、HuggingFaceからモデルをダウンロードし、
ターミナルでそのモデルと対話したい。
ollamaと同様の操作感で、学習コストを最小限に抑えたい。

**この優先度の理由**: モデルの取得と対話は最も基本的な機能であり、
他のすべての機能の前提となる。

**独立テスト**: `llmlb node pull <model>` でモデルをダウンロードし、
`llmlb node run <model>` で対話できることで完全にテスト可能。

**受け入れシナリオ**:

1. **前提** HuggingFaceにモデルが存在する、
   **実行** `llmlb node pull meta-llama/Llama-3.2-3B-Instruct`、
   **結果** プログレスバーが表示され、モデルがローカルに保存される
2. **前提** モデルがローカルに存在する、
   **実行** `llmlb node run meta-llama/Llama-3.2-3B-Instruct`、
   **結果** REPLが起動し、プロンプトに入力するとモデルが応答する
3. **前提** REPLが起動中、
   **実行** `/bye` を入力、
   **結果** REPLが終了し、コマンドプロンプトに戻る

---

### ユーザーストーリー2 - モデルの管理 (優先度: P1)

ユーザーとして、ローカルにあるモデルの一覧確認、詳細表示、削除を行いたい。
ストレージ管理のために不要なモデルを削除できる必要がある。

**この優先度の理由**: モデル管理は運用上必須の機能であり、
ストレージ容量の制約があるローカル環境では特に重要。

**独立テスト**: `list`, `show`, `rm` コマンドの動作確認で完全にテスト可能。

**受け入れシナリオ**:

1. **前提** 複数のモデルがローカルに存在する、
   **実行** `llmlb node list`、
   **結果** モデル名、サイズ、最終使用日時が一覧表示される
2. **前提** モデルがローカルに存在する、
   **実行** `llmlb node show <model>`、
   **結果** アーキテクチャ、パラメータ数、量子化情報等が表示される
3. **前提** モデルがローカルに存在する、
   **実行** `llmlb node rm <model>`、
   **結果** 確認なしで即座に削除され、ストレージが解放される

---

### ユーザーストーリー3 - サーバーモードでの運用 (優先度: P1)

ユーザーとして、ノードをサーバーとして起動し、
他のコマンドやアプリケーションからAPIでアクセスしたい。

**この優先度の理由**: サーバーモードは他のすべてのコマンドの基盤となり、
API経由でのアクセスを可能にする。

**独立テスト**: `serve` 起動後に `run`, `list` 等が動作することで検証可能。

**受け入れシナリオ**:

1. **前提** ノードが未起動、
   **実行** `llmlb node serve`、
   **結果** フォアグラウンドでサーバーが起動し、ログが標準出力される
2. **前提** サーバーが起動中、
   **実行** 別ターミナルで `llmlb node list`、
   **結果** サーバーに接続してモデル一覧を取得できる
3. **前提** サーバーが起動中、
   **実行** Ctrl+C、
   **結果** グレースフルにシャットダウンされる

---

### ユーザーストーリー4 - 実行中モデルの監視と制御 (優先度: P2)

ユーザーとして、現在ロード中のモデルの状態を確認し、
不要なモデルを停止してVRAMを解放したい。

**この優先度の理由**: GPU リソース管理は重要だが、
基本的なモデル操作の後に必要になる機能。

**独立テスト**: `ps`, `stop` コマンドで実行中モデルの確認と停止が可能。

**受け入れシナリオ**:

1. **前提** モデルがロードされている、
   **実行** `llmlb node ps`、
   **結果** NAME, ID, SIZE, PROCESSOR, UNTIL, VRAM使用率, 温度が表示される
2. **前提** モデルがロードされている、
   **実行** `llmlb node stop <model>`、
   **結果** モデルがアンロードされ、VRAMが解放される

---

### ユーザーストーリー5 - ロードバランサー経由のクラスタ管理 (優先度: P2)

ユーザーとして、ロードバランサーを通じて複数ノードを管理し、
モデルの配信状況を確認したい。

**この優先度の理由**: 分散構成での運用に必要だが、
単体ノード機能が完成した後に対応可能。

**独立テスト**: `llmlb router` サブコマンドでクラスタ状態を確認可能。

**受け入れシナリオ**:

1. **前提** ロードバランサーが起動中、
   **実行** `llmlb router nodes`、
   **結果** 登録されているノード一覧と状態が表示される
2. **前提** ロードバランサーが起動中、
   **実行** `llmlb router models`、
   **結果** 各ノードで利用可能なモデル一覧が表示される
3. **前提** ロードバランサーが起動中、
   **実行** `llmlb router status`、
   **結果** クラスタ全体の状態サマリーが表示される

---

### ユーザーストーリー6 - Vision入力でのマルチモーダル対話 (優先度: P2)

ユーザーとして、REPLで画像を含めた対話を行いたい。
Vision対応モデルで画像の内容について質問できる必要がある。

**この優先度の理由**: マルチモーダル機能は付加価値だが、
テキスト対話が完成した後に対応可能。

**独立テスト**: 画像パスを含むプロンプトでモデルが応答することで検証可能。

**受け入れシナリオ**:

1. **前提** Vision対応モデルがロード済み、REPLが起動中、
   **実行** `/path/to/image.png この画像について説明して`、
   **結果** モデルが画像の内容を分析して応答する

---

### ユーザーストーリー7 - Reasoningモデルでの思考過程表示 (優先度: P2)

ユーザーとして、DeepSeek-R1等のreasoningモデルで思考過程を
表示または非表示にできるようにしたい。

**この優先度の理由**: reasoning機能は特定モデル向けの機能であり、
基本的な対話機能の後に対応可能。

**独立テスト**: `--think` フラグの有無で出力が変化することで検証可能。

**受け入れシナリオ**:

1. **前提** reasoningモデルがロード済み、
   **実行** `llmlb node run <model> --think`、
   **結果** 思考過程が表示された後に回答が表示される
2. **前提** reasoningモデルがロード済み、
   **実行** `llmlb node run <model> --hidethinking`、
   **結果** 思考過程は非表示で回答のみ表示される

---

### ユーザーストーリー8 - ollamaモデルの参照 (優先度: P3)

ユーザーとして、既にollamaでダウンロード済みのモデルを
再ダウンロードせずに利用したい。

**この優先度の理由**: ストレージ節約のための付加機能であり、
コア機能完成後に対応可能。

**独立テスト**: ollamaのモデルディレクトリを参照して利用可能かで検証。

**受け入れシナリオ**:

1. **前提** ollamaでモデルがダウンロード済み、
   **実行** `llmlb node list`、
   **結果** ollamaのモデルも一覧に表示される（読み取り専用として）
2. **前提** ollamaにのみモデルが存在する、
   **実行** `llmlb node run ollama:llama3.2`、
   **結果** ollamaのモデルを参照して対話できる

---

### エッジケース

- gatedモデル（認証必要）でHF_TOKEN未設定の場合、
  明確なエラーメッセージで即座に終了する
- サーバー未起動時にクライアントコマンドを実行した場合、
  サーバーへの接続エラーを表示する
- ディスク容量不足時のpull実行で、
  ダウンロード開始前にエラーを表示する
- 存在しないモデルに対するrun/show/rm実行で、
  モデルが見つからない旨のエラーを表示する
- 不正なモデル名（特殊文字等）の入力時、
  バリデーションエラーを表示する

## 要件 *(必須)*

### 機能要件

#### nodeサブコマンド

- **FR-001**: `serve` コマンドはフォアグラウンドでHTTPサーバーを起動する
- **FR-002**: `run` コマンドはREPLを起動し、`/bye`, `/clear` コマンドに対応する
- **FR-003**: `run` コマンドはvision入力（画像パス指定）をサポートする
- **FR-004**: `run` コマンドは `--think` フラグでreasoning出力を制御できる
- **FR-005**: `pull` コマンドはHuggingFace URL またはモデル名でダウンロードする
- **FR-006**: `pull` コマンドはollama互換のプログレス表示を行う
- **FR-007**: `list` コマンドはローカルモデルの一覧を表示する
- **FR-008**: `show` コマンドはモデルのメタデータを表示する
- **FR-009**: `show` コマンドは `--license`, `--parameters` 等のサブオプションに対応
- **FR-010**: `rm` コマンドは確認なしでモデルを削除する
- **FR-011**: `stop` コマンドは実行中のモデルをアンロードする
- **FR-012**: `ps` コマンドは実行中モデルとVRAM使用率、温度を表示する

#### routerサブコマンド

- **FR-013**: `router nodes` コマンドはノード一覧を表示する
- **FR-014**: `router models` コマンドは各ノードのモデル状態を表示する
- **FR-015**: `router status` コマンドはクラスタ状態を表示する

#### 共通機能

- **FR-016**: 環境変数 `LLMLB_HOST` でサーバー接続先を指定できる
- **FR-017**: 環境変数 `LLMLB_DEBUG` でログレベルを制御できる
- **FR-018**: エイリアス機能でモデルを短縮名で指定できる（pull時に自動生成）
- **FR-019**: ollamaの `~/.ollama/models/` を読み取り専用で参照できる
- **FR-020**: 終了コードは 0=成功, 1=一般エラー, 2=接続エラー で統一する
- **FR-021**: エラーメッセージは英語で出力する

### 主要エンティティ

- **Model**: ローカルに保存されたLLMモデル。名前、パス、サイズ、
  アーキテクチャ、量子化情報、エイリアスを持つ
- **Node**: 推論を実行するサーバー。ホスト、ポート、状態、
  ロード中モデル、VRAM使用状況を持つ
- **Session**: REPLの対話セッション。モデル、履歴、設定を持つ

---

## スコープ外 *(オプション)*

以下の機能は本仕様のスコープ外とし、将来のバージョンで対応予定:

- `push` コマンド（レジストリへのアップロード）
- `--insecure` フラグ（HTTPS証明書検証スキップ）
- ローカルファイルパスからのモデルインポート
- i18n対応（多言語エラーメッセージ）
- デーモン化（systemd/launchd統合）

---

## 技術制約 *(該当する場合)*

- サーバーモデル: `serve` 常駐、他コマンドはクライアントとして接続
- ollamaモデル参照: manifest解析してblob参照（読み取り専用）
- REPL: /bye, /clear のみ対応（/set, /load, /save は対象外）

---

## 前提条件 *(該当する場合)*

この機能は以下を前提とします:

- llmlb nodeバイナリがビルド済みであること
- GPUが搭載されていること（推論実行のため）
- HuggingFaceアカウントがあること（gatedモデル利用時）

---

## 依存関係 *(該当する場合)*

この機能は以下に依存します:

- SPEC-69549000: safetensors.cppサポート（モデル読み込み）
- 既存のモデルダウンロード機能（llmlb/src/cli/）
- 既存のHTTP API（/v1/chat/completions等）

---

## 成功基準 *(必須)*

以下の成功基準を満たす必要があります:

1. ollamaユーザーが学習コスト30分以内で基本操作を習得できる
2. `pull` から `run` までの操作が5分以内に完了できる（ネットワーク時間除く）
3. REPLの応答開始までのレイテンシがollamaと同等（±10%以内）
4. `list`, `ps`, `show` の応答が1秒以内に完了する
5. すべてのコマンドのヘルプが `--help` で表示できる
